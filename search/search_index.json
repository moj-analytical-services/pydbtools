{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"pydbtools","text":"<p>A package that is used to run SQL queries speficially configured for the Analytical Platform. This packages uses AWS Wrangler's Athena module but adds additional functionality (like Jinja templating, creating temporary tables) and alters some configuration to our specification.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires a pip release above 20.</p> <pre><code>## To install from pypi\npip install pydbtools\n\n##\u00a0Or install from git with a specific release\npip install \"pydbtools @ git+https://github.com/moj-analytical-services/pydbtools@v4.0.1\"\n</code></pre>"},{"location":"#quickstart-guide","title":"Quickstart guide","text":"<p>The examples directory contains more detailed notebooks demonstrating the use of this library, many of which are borrowed from the mojap-aws-tools-demo repo.</p>"},{"location":"#read-an-sql-athena-query-into-a-pandas-dataframe","title":"Read an SQL Athena query into a pandas dataframe","text":"<pre><code>import pydbtools as pydb\ndf = pydb.read_sql_query(\"SELECT * from a_database.table LIMIT 10\")\n</code></pre>"},{"location":"#run-a-query-in-athena","title":"Run a query in Athena","text":"<pre><code>response = pydb.start_query_execution_and_wait(\"CREATE DATABASE IF NOT EXISTS my_test_database\")\n</code></pre>"},{"location":"#create-a-temporary-table-to-do-further-separate-sql-queries-on-later","title":"Create a temporary table to do further separate SQL queries on later","text":"<pre><code>pydb.create_temp_table(\"SELECT a_col, count(*) as n FROM a_database.table GROUP BY a_col\", table_name=\"temp_table_1\")\ndf = pydb.read_sql_query(\"SELECT * FROM __temp__.temp_table_1 WHERE n &lt; 10\")\n\npydb.dataframe_to_temp_table(my_dataframe, \"my_table\")\ndf = pydb.read_sql_query(\"select * from __temp__.my_table where year = 2022\")\n</code></pre>"},{"location":"#notes","title":"Notes","text":"<ul> <li>Amazon Athena using a flavour of SQL called trino. Docs can be found here</li> <li>To query a date column in Athena you need to specify that your value is a date e.g. <code>SELECT * FROM db.table WHERE date_col &gt; date '2018-12-31'</code></li> <li>To query a datetime or timestamp column in Athena you need to specify that your value is a timestamp e.g. <code>SELECT * FROM db.table WHERE datetime_col &gt; timestamp '2018-12-31 23:59:59'</code></li> <li>Note dates and datetimes formatting used above. See more specifics around date and datetimes here</li> <li>To specify a string in the sql query always use '' not \"\". Using \"\"'s means that you are referencing a database, table or col, etc.</li> <li>If you are working in an environment where you cannot change the default AWS region environment variables you can set <code>AWS_ATHENA_QUERY_REGION</code> which will override these.</li> <li>You can override the bucket where query results are outputted to with the <code>ATHENA_QUERY_DUMP_BUCKET</code> environment variable. This is mandatory if you set the region to something other than <code>eu-west-1</code>.</li> </ul> <p>See changelog for release changes.</p>"},{"location":"api/deprecated/","title":"Deprecated","text":"<p>The functions:</p> <ul> <li> <p><code>pydbtools.get_athena_query_response</code></p> </li> <li> <p><code>pydbtools.read_sql</code></p> </li> </ul> <p>Are now deprecated and calls to these functions will raise an warning. They have been replaced by <code>pydbtools.start_query_execution_and_wait</code> and <code>pydbtools.read_sql_query</code>.</p>"},{"location":"api/sql_render/","title":"SQL render","text":""},{"location":"api/sql_render/#pydbtools._sql_render.get_sql_from_file","title":"<code>get_sql_from_file(filepath, jinja_args=None, **kwargs)</code>","text":"<p>Read in an SQL file and inject arguments with Jinja (if given params). Returns the SQL as a str.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>A filepath to your SQL file.</p> required <code>jinja_args</code> <code>dict</code> <p>If not None, will pass the read in SQL file through a jinja template to render the template. Otherwise will just return the SQL file as is. Defaults to None.</p> <code>None</code> <code>kwargs</code> <p>passed to the open() call.</p> <code>{}</code> Source code in <code>pydbtools/_sql_render.py</code> <pre><code>def get_sql_from_file(filepath: str, jinja_args: dict = None, **kwargs) -&gt; str:\n    \"\"\"\n    Read in an SQL file and inject arguments with Jinja (if given params).\n    Returns the SQL as a str.\n\n    Args:\n        filepath (str): A filepath to your SQL file.\n        jinja_args (dict, optional): If not None, will pass the read\n            in SQL file through a jinja template to render the template.\n            Otherwise will just return the SQL file as is. Defaults to None.\n        kwargs: passed to the open() call.\n    \"\"\"\n    with open(filepath, **kwargs) as f:\n        sql = \"\".join(f.readlines())\n    if jinja_args:\n        sql = render_sql_template(sql, jinja_args)\n    return sql\n</code></pre>"},{"location":"api/sql_render/#pydbtools._sql_render.render_sql_template","title":"<code>render_sql_template(sql, jinja_args)</code>","text":"<p>Takes a SQL file templated with Jinja and then injects arguments. Returns the injected SQL.</p> <p>Parameters:</p> Name Type Description Default <code>sql_file</code> <code>str</code> <p>Path to SQL file</p> required <code>args</code> <code>dict</code> <p>Arguments that is referenced in the SQL file</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>SQL string that has args rendered into it</p> Source code in <code>pydbtools/_sql_render.py</code> <pre><code>def render_sql_template(sql: str, jinja_args: dict) -&gt; str:\n    \"\"\"\n    Takes a SQL file templated with Jinja and then injects arguments.\n    Returns the injected SQL.\n\n    Args:\n        sql_file (str): Path to SQL file\n        args (dict): Arguments that is referenced in the SQL file\n\n    Returns:\n        str: SQL string that has args rendered into it\n    \"\"\"\n    return Template(sql).render(**jinja_args)\n</code></pre>"},{"location":"api/utils/","title":"Utilities","text":""},{"location":"api/utils/#pydbtools.utils.check_temp_query","title":"<code>check_temp_query(sql)</code>","text":"<p>Checks if a query to a temporary table has had temp wrapped in quote marks.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>an SQL query</p> required Source code in <code>pydbtools/utils.py</code> <pre><code>def check_temp_query(sql: str):\n    \"\"\"\n    Checks if a query to a temporary table\n    has had __temp__ wrapped in quote marks.\n\n    Args:\n        sql (str): an SQL query\n\n    Raises:\n        ValueError\n    \"\"\"\n    if re.findall(r'[\"|\\']__temp__[\"|\\']\\.', sql.lower()):\n        raise ValueError(\n            \"When querying a temporary database, \"\n            \"__temp__ should not be wrapped in quotes\"\n        )\n</code></pre>"},{"location":"api/utils/#pydbtools.utils.clean_query","title":"<code>clean_query(sql, fmt_opts=None)</code>","text":"<p>removes trailing whitespace, newlines and final semicolon from sql for use with sqlparse package Args:     sql (str): The raw SQL query     fmt_opts (dict): Dictionary of params to pass to sqlparse.format.     If None then sqlparse.format is not called. Returns:     str: The cleaned SQL query</p> Source code in <code>pydbtools/utils.py</code> <pre><code>def clean_query(sql: str, fmt_opts: Optional[dict] = None) -&gt; str:\n    \"\"\"\n    removes trailing whitespace, newlines and final\n    semicolon from sql for use with\n    sqlparse package\n    Args:\n        sql (str): The raw SQL query\n        fmt_opts (dict): Dictionary of params to pass to sqlparse.format.\n        If None then sqlparse.format is not called.\n    Returns:\n        str: The cleaned SQL query\n    \"\"\"\n    if fmt_opts is None:\n        fmt_opts = {}\n    fmt_opts[\"strip_comments\"] = True\n    sql = sqlparse.format(sql, **fmt_opts)\n    sql = \" \".join(sql.splitlines()).strip().rstrip(\";\")\n    return sql\n</code></pre>"},{"location":"api/utils/#pydbtools.utils.replace_temp_database_name_reference","title":"<code>replace_temp_database_name_reference(sql, database_name)</code>","text":"<p>Replaces references to the user's temp database temp with the database_name string provided.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The raw SQL query as a string</p> required <code>database_name</code> <code>str</code> <p>The database name to replace temp</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The new SQL query which is sent to Athena</p> Source code in <code>pydbtools/utils.py</code> <pre><code>def replace_temp_database_name_reference(sql: str, database_name: str) -&gt; str:\n    \"\"\"\n    Replaces references to the user's temp database __temp__\n    with the database_name string provided.\n\n    Args:\n        sql (str): The raw SQL query as a string\n        database_name (str): The database name to replace __temp__\n\n    Returns:\n        str: The new SQL query which is sent to Athena\n    \"\"\"\n\n    parsed = sqlparse.parse(sql)\n    new_query = []\n    for query in parsed:\n        check_temp_query(str(query))\n        # Get all the separated tokens from subtrees\n        fq = list(query.flatten())\n        # Join them back together replacing __temp__\n        # where necessary\n        new_query.append(\n            \"\".join(\n                re.sub(\"^__temp__\", database_name, str(word), flags=re.IGNORECASE)\n                for word in fq\n            )\n        )\n    # Strip output for consistency, different versions of sqlparse\n    # treat a trailing newline differently\n    return \"\".join(new_query).strip()\n</code></pre>"},{"location":"api/utils/#pydbtools.utils.get_database_name_from_sql","title":"<code>get_database_name_from_sql(sql)</code>","text":"<p>Obtains database name from SQL query for use by awswrangler.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The raw SQL query as a string</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The database table name</p> Source code in <code>pydbtools/utils.py</code> <pre><code>def get_database_name_from_sql(sql: str) -&gt; str:\n    \"\"\"\n    Obtains database name from SQL query for use\n    by awswrangler.\n\n    Args:\n        sql (str): The raw SQL query as a string\n\n    Returns:\n        str: The database table name\n    \"\"\"\n\n    for table in sql_metadata.Parser(sql).tables:\n        # Return the first database seen in the\n        # form \"database.table\"\n        xs = table.split(\".\")\n        if len(xs) &gt; 1:\n            return xs[0]\n\n    # Return default in case of failure to parse\n    return None\n</code></pre>"},{"location":"api/wrangler/","title":"<code>awswrangler</code> extensions","text":""},{"location":"api/wrangler/#pydbtools._wrangler.init_athena_params","title":"<code>init_athena_params(func=None, *, allow_boto3_session=False)</code>","text":"<p>Takes a wrangler athena function and sets the following: boto3_session and s3_output_path if exists in function param.</p> <p>Parameters:</p> Name Type Description Default <code>func</code> <code>Callable</code> <p>An function from wr.athena that requires</p> <code>None</code> <p>Returns:</p> Type Description <p>Similar function call but with pre-defined params.</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def init_athena_params(func=None, *, allow_boto3_session=False):  # noqa: C901\n    \"\"\"\n    Takes a wrangler athena function and sets the following:\n    boto3_session and s3_output_path if exists in function param.\n\n    Args:\n        func (Callable): An function from wr.athena that requires\n        boto3_session. If the func has an s3_output this is also\n        standardised.\n\n    Returns:\n        Similar function call but with pre-defined params.\n    \"\"\"\n    # Allows parameterisation of this wrapper fun\n    if func is None:\n        return functools.partial(\n            init_athena_params, allow_boto3_session=allow_boto3_session\n        )\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Get parameters from function and overwrite specific params\n        sig = inspect.signature(func)\n        argmap = sig.bind_partial(*args, **kwargs).arguments\n\n        # Create a db flag\n        database_flag = \"database\" in sig.parameters and (\n            argmap.get(\"database\", \"__temp__\") in [\"__temp__\", \"__TEMP__\"]\n        )\n\n        # If wrapper allows boto3 session being defined by user\n        # and it has been then do not create new boto3 session\n        # otherwise do\n        if allow_boto3_session and argmap.get(\"boto3_session\"):\n            pass\n        else:\n            # Get the boto3 session\n            setup_defaults = get_default_args(get_boto_session)\n            setup_kwargs = {}\n            for k, v in setup_defaults.items():\n                setup_kwargs[k] = kwargs.pop(k, v)\n            boto3_session = get_boto_session(**setup_kwargs)\n\n            if argmap.get(\"boto3_session\") is not None:\n                warn_msg = (\n                    \"Warning parameter 'boto3_session' cannot be set. \"\n                    \"Is defined by setting 'force_ec2' and 'region' params. \"\n                    \"(Input boto3_session will be ignored).\"\n                )\n                warnings.warn(warn_msg)\n            argmap[\"boto3_session\"] = boto3_session\n\n        # Set s3 table path and get temp_db_name\n        if (\n            (\"s3_output\" in sig.parameters)\n            or (\"sql\" in sig.parameters)\n            or database_flag\n        ):\n            user_id, s3_output = get_user_id_and_table_dir(boto3_session)\n            temp_db_name = get_database_name_from_userid(user_id)\n\n        # Set s3_output to predefined path otherwise skip\n        if \"s3_output\" in sig.parameters:\n            if argmap.get(\"s3_output\") is not None:\n                warn_msg = (\n                    \"Warning parameter 's3_output' cannot be set. \"\n                    \"Is automatically generated (input ignored).\"\n                )\n                warnings.warn(warn_msg)\n\n            # Set s3 to default s3 path\n            argmap[\"s3_output\"] = s3_output\n\n        # Set ctas_approach to True if not set.\n        # Although awswrangler does this by default, we want to ensure\n        # that timestamps are read in correctly to pandas using pyarrow.\n        # Therefore forcing the default option to be True in case future\n        # versions of wrangler change their default behaviour.\n        if \"ctas_approach\" in sig.parameters and argmap.get(\"ctas_approach\") is None:\n            argmap[\"ctas_approach\"] = True\n\n        # Set database to None or set to keyword temp when not needed\n        if database_flag:\n            if \"ctas_approach\" in sig.parameters and argmap[\"ctas_approach\"]:\n                argmap[\"database\"] = temp_db_name\n                _ = _create_temp_database(temp_db_name, boto3_session=boto3_session)\n            elif argmap.get(\"database\", \"\").lower() == \"__temp__\":\n                argmap[\"database\"] = temp_db_name\n            else:\n                argmap[\"database\"] = None\n\n        # Fix sql before it is passed to athena\n        if \"sql\" in argmap:\n            argmap[\"sql\"] = replace_temp_database_name_reference(\n                argmap[\"sql\"], temp_db_name\n            )\n\n        if (\n            \"sql\" in sig.parameters\n            and \"database\" in sig.parameters\n            and argmap.get(\"database\") is None\n        ):\n            argmap[\"database\"] = get_database_name_from_sql(argmap.get(\"sql\", \"\"))\n\n        # Set pyarrow_additional_kwargs\n        if (\n            \"pyarrow_additional_kwargs\" in argmap\n            and argmap.get(\"pyarrow_additional_kwargs\", None) is None\n        ):\n            argmap[\"pyarrow_additional_kwargs\"] = {\n                \"coerce_int96_timestamp_unit\": \"ms\",\n                \"timestamp_as_object\": True,\n            }\n\n        logger.debug(f\"Modifying function {func.__name__}\")\n        logger.debug(pprint.pformat(dict(argmap)))\n        return func(**argmap)\n\n    return wrapper\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.start_query_execution_and_wait","title":"<code>start_query_execution_and_wait(sql, *args, **kwargs)</code>","text":"<p>Calls start_query_execution followed by wait_query. args and *kwargs are passed to start_query_execution</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>An SQL string. Which works with TEMP references.</p> required Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params\ndef start_query_execution_and_wait(sql, *args, **kwargs):\n    \"\"\"Calls start_query_execution followed by wait_query.\n    *args and **kwargs are passed to start_query_execution\n\n    Args:\n        sql (str): An SQL string. Which works with __TEMP__ references.\n    \"\"\"\n\n    # Function wrapper is applied to top of function so we need\n    # to call the original unwrapped athena fun to ensure the wrapper fun\n    # is not called again\n    query_execution_id = ath.start_query_execution(sql, *args, **kwargs)\n    return ath.wait_query(query_execution_id, boto3_session=kwargs.get(\"boto3_session\"))\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.check_sql","title":"<code>check_sql(sql)</code>","text":"<p>Validates sql to confirm it is a select statement</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def check_sql(sql: str):\n    \"\"\"\n    Validates sql to confirm it is a select statement\n    \"\"\"\n    parsed = sqlparse.parse(clean_query(sql))\n    i = 0\n    for p in parsed:\n        if p.get_type() != \"SELECT\" or i &gt; 0:\n            raise ValueError(\"The sql statement must be a single select query\")\n        i += 1\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.create_temp_table","title":"<code>create_temp_table(sql, table_name, boto3_session=None, force_ec2=False, region_name=None)</code>","text":"<p>Create a table inside the temporary database from create table</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL table you want to create a temp table out of. Should be a table that starts with a WITH or SELECT clause.</p> required <code>table_name</code> <code>str</code> <p>The name of the temp table you wish to create</p> required <code>force_ec2</code> <code>bool</code> <p>Boolean specifying if the user wants to force boto to get the credentials from the EC2. This is for dbtools which is the R wrapper that calls this package via reticulate and requires credentials to be refreshed via the EC2 instance (and therefore sets this to True) - this is not necessary when using this in Python. Default is False.</p> <code>False</code> <code>region_name</code> <code>str</code> <p>Name of the AWS region you want to run queries on. Defaults to pydbtools.utils.aws_default_region (which if left unset is \"eu-west-1\").</p> <code>None</code> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params\ndef create_temp_table(\n    sql: str,\n    table_name: str,\n    boto3_session=None,\n    force_ec2: bool = False,\n    region_name: str = None,\n):\n    \"\"\"\n    Create a table inside the temporary database from create table\n\n    Args:\n        sql (str):\n            The SQL table you want to create a temp table out of. Should\n            be a table that starts with a WITH or SELECT clause.\n\n        table_name (str):\n            The name of the temp table you wish to create\n\n        force_ec2 (bool, optional):\n            Boolean specifying if the user wants to force boto to get the\n            credentials from the EC2. This is for dbtools which is the R\n            wrapper that calls this package via reticulate and requires\n            credentials to be refreshed via the EC2 instance (and\n            therefore sets this to True) - this is not\n            necessary when using this in Python. Default is False.\n\n        region_name (str, optional):\n            Name of the AWS region you want to run queries on. Defaults to\n            pydbtools.utils.aws_default_region (which if left unset is\n            \"eu-west-1\").\n    \"\"\"\n    region_name = _set_region_name(region_name)\n    check_sql(sql)\n\n    # Create named stuff\n    user_id, out_path = get_user_id_and_table_dir(boto3_session=boto3_session)\n    db_path = os.path.join(out_path, \"__athena_temp_db__/\")\n    # Include timestamp in path to avoid permissions problems with\n    # previous sessions\n    ts = str(time.time()).replace(\".\", \"\")\n    table_path = os.path.join(db_path, ts, table_name)\n    temp_db_name = get_database_name_from_userid(user_id)\n\n    _ = create_temp_database(temp_db_name, boto3_session=boto3_session)\n\n    # Clear out table every time, making sure other tables aren't being\n    # cleared out\n    delete_temp_table(table_name, boto3_session=boto3_session)\n\n    ctas_query = f\"\"\"\n    CREATE TABLE {temp_db_name}.{table_name}\n        WITH (\n            format = 'Parquet',\n            parquet_compression  = 'SNAPPY',\n            external_location = '{table_path}'\n        )\n    as {sql}\n    \"\"\"\n\n    q_e_id = ath.start_query_execution(ctas_query, boto3_session=boto3_session)\n\n    ath.wait_query(q_e_id, boto3_session=boto3_session)\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.create_table","title":"<code>create_table(sql, database, table, location, partition_cols=None, boto3_session=None)</code>","text":"<p>Create a table in a database from a SELECT statement</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL starting with a WITH or SELECT clause</p> required <code>database</code> <code>str</code> <p>Database name</p> required <code>table</code> <code>str</code> <p>Table name</p> required <code>location</code> <code>str</code> <p>S3 path to where the table should be stored</p> required <code>partition_cols</code> <code>List[str]</code> <p>partition columns (optional)</p> <code>None</code> <code>boto3_session</code> <p>optional boto3 session</p> <code>None</code> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def create_table(\n    sql: str,\n    database: str,\n    table: str,\n    location: str,\n    partition_cols: Optional[List[str]] = None,\n    boto3_session=None,\n):\n    \"\"\"\n    Create a table in a database from a SELECT statement\n\n    Args:\n        sql (str): SQL starting with a WITH or SELECT clause\n        database (str): Database name\n        table (str): Table name\n        location (str): S3 path to where the table should be stored\n        partition_cols (List[str]): partition columns (optional)\n        boto3_session: optional boto3 session\n    \"\"\"\n    return ath.create_ctas_table(\n        sql=sql,\n        database=database,\n        ctas_database=database,\n        ctas_table=table,\n        s3_output=s3_path_join(location, table + \".parquet\"),\n        partitioning_info=partition_cols,\n        wait=True,\n        boto3_session=boto3_session,\n    )\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.read_sql_queries","title":"<code>read_sql_queries(sql)</code>","text":"<p>Reads a number of SQL statements and returns the result of the last select statement as a dataframe. Temporary tables can be created using CREATE TEMP TABLE tablename AS (sql query) and accessed using temp as the database.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL commands</p> required <p>Returns:</p> Type Description <code>Optional[DataFrame]</code> <p>An iterator of Pandas DataFrames.</p> Example <p>If the file eg.sql contains the SQL code     create temp table A as (         select * from database.table1         where year = 2021     );</p> <pre><code>create temp table B as (\n    select * from database.table2\n    where amount &gt; 10\n);\n\nselect * from __temp__.A\nleft join __temp__.B\non A.id = B.id;\n</code></pre> <p>df = read_sql_queries(open('eg.sql', 'r').read())</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def read_sql_queries(sql: str) -&gt; Optional[pd.DataFrame]:\n    \"\"\"\n    Reads a number of SQL statements and returns the result of\n    the last select statement as a dataframe.\n    Temporary tables can be created using\n    CREATE TEMP TABLE tablename AS (sql query)\n    and accessed using __temp__ as the database.\n\n    Args:\n        sql (str): SQL commands\n\n    Returns:\n        An iterator of Pandas DataFrames.\n\n    Example:\n        If the file eg.sql contains the SQL code\n            create temp table A as (\n                select * from database.table1\n                where year = 2021\n            );\n\n            create temp table B as (\n                select * from database.table2\n                where amount &gt; 10\n            );\n\n            select * from __temp__.A\n            left join __temp__.B\n            on A.id = B.id;\n\n        df = read_sql_queries(open('eg.sql', 'r').read())\n    \"\"\"\n\n    df = None\n    for df in read_sql_queries_gen(sql):\n        pass\n    return df\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.read_sql_queries_gen","title":"<code>read_sql_queries_gen(sql)</code>","text":"<p>Reads a number of SQL statements and returns the result of any select statements as a dataframe generator. Temporary tables can be created using CREATE TEMP TABLE tablename AS (sql query) and accessed using temp as the database.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>SQL commands</p> required <p>Returns:</p> Type Description <code>Iterator[DataFrame]</code> <p>An iterator of Pandas DataFrames.</p> Example <p>If the file eg.sql contains the SQL code     create temp table A as (         select * from database.table1         where year = 2021     );</p> <pre><code>create temp table B as (\n    select * from database.table2\n    where amount &gt; 10\n);\n\nselect * from __temp__.A\nleft join __temp__.B\non A.id = B.id;\n\nselect * from __temp__.A\nwhere country = 'UK'\n</code></pre> <p>df_iter = read_sql_queries(open('eg.sql', 'r').read()) df1 = next(df_iter) df2 = next(df_iter)</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def read_sql_queries_gen(sql: str) -&gt; Iterator[pd.DataFrame]:\n    \"\"\"\n    Reads a number of SQL statements and returns the result of\n    any select statements as a dataframe generator.\n    Temporary tables can be created using\n    CREATE TEMP TABLE tablename AS (sql query)\n    and accessed using __temp__ as the database.\n\n    Args:\n        sql (str): SQL commands\n\n    Returns:\n        An iterator of Pandas DataFrames.\n\n    Example:\n        If the file eg.sql contains the SQL code\n            create temp table A as (\n                select * from database.table1\n                where year = 2021\n            );\n\n            create temp table B as (\n                select * from database.table2\n                where amount &gt; 10\n            );\n\n            select * from __temp__.A\n            left join __temp__.B\n            on A.id = B.id;\n\n            select * from __temp__.A\n            where country = 'UK'\n\n        df_iter = read_sql_queries(open('eg.sql', 'r').read())\n        df1 = next(df_iter)\n        df2 = next(df_iter)\n    \"\"\"\n\n    for query in sqlparse.parse(sql):\n        if not _create_temp_table_in_sql(str(query)):\n            if query.get_type() == \"SELECT\":\n                yield read_sql_query(str(query))\n            else:\n                start_query_execution_and_wait(str(query))\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.delete_table_and_data","title":"<code>delete_table_and_data(table, database, boto3_session=None)</code>","text":"<p>Deletes both a table from an Athena database and the underlying data on S3.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table name to drop.</p> required <code>database</code> <code>str</code> <p>The database name.</p> required <p>Returns:</p> Type Description <p>True if table exists and is deleted, False if table</p> <p>does not exist</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef delete_table_and_data(table: str, database: str, boto3_session=None):\n    \"\"\"\n    Deletes both a table from an Athena database and the underlying data on S3.\n\n    Args:\n        table (str): The table name to drop.\n        database (str): The database name.\n\n    Returns:\n        True if table exists and is deleted, False if table\n        does not exist\n    \"\"\"\n\n    if table in list(tables(database=database, limit=None)[\"Table\"]):\n        path = get_table_location(\n            database=database, table=table, boto3_session=boto3_session\n        )\n        wr.s3.delete_objects(path, boto3_session=boto3_session)\n        wr.catalog.delete_table_if_exists(\n            database=database, table=table, boto3_session=boto3_session\n        )\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.delete_temp_table","title":"<code>delete_temp_table(table, boto3_session=None)</code>","text":"<p>Deletes a temporary table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table name to drop.</p> required <p>Returns:</p> Type Description <p>True if table exists and is deleted, False if table</p> <p>does not exist</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef delete_temp_table(table: str, boto3_session=None):\n    \"\"\"\n    Deletes a temporary table.\n\n    Args:\n        table (str): The table name to drop.\n\n    Returns:\n        True if table exists and is deleted, False if table\n        does not exist\n    \"\"\"\n\n    user_id, table_dir = get_user_id_and_table_dir(boto3_session=boto3_session)\n    database = get_database_name_from_userid(user_id)\n    _create_temp_database(database, boto3_session=boto3_session)\n\n    if table in list(tables(database=database, limit=None)[\"Table\"]):\n        path = get_table_location(\n            database=database, table=table, boto3_session=boto3_session\n        )\n\n        # Use try in case table was set up in previous session\n        try:\n            wr.s3.delete_objects(path, boto3_session=boto3_session)\n        except wr.exceptions.ServiceApiError:\n            pass\n\n        wr.catalog.delete_table_if_exists(\n            database=database, table=table, boto3_session=boto3_session\n        )\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.delete_database_and_data","title":"<code>delete_database_and_data(database, boto3_session=None)</code>","text":"<p>Deletes both an Athena database and the underlying data on S3.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The database name to drop.</p> required <p>Returns:</p> Type Description <p>True if database exists and is deleted, False if database</p> <p>does not exist</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef delete_database_and_data(database: str, boto3_session=None):\n    \"\"\"\n    Deletes both an Athena database and the underlying data on S3.\n\n    Args:\n        database (str): The database name to drop.\n\n    Returns:\n        True if database exists and is deleted, False if database\n        does not exist\n    \"\"\"\n    if database not in (db[\"Name\"] for db in wr.catalog.get_databases()):\n        return False\n    for table in wr.catalog.get_tables(database=database, boto3_session=boto3_session):\n        delete_table_and_data(table[\"Name\"], database, boto3_session=boto3_session)\n    wr.catalog.delete_database(database, boto3_session=boto3_session)\n    return True\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.delete_partitions_and_data","title":"<code>delete_partitions_and_data(database, table, expression, boto3_session=None)</code>","text":"<p>Deletes partitions and the underlying data on S3 from an Athena database table matching an expression.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The database name.</p> required <code>table</code> <code>str</code> <p>The table name.</p> required <code>expression</code> <code>str</code> <p>The expression to match.</p> required <p>Please see https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_partitions # noqa for instructions on the expression construction, but at a basic level you can use SQL syntax on your partition columns.</p> <p>Examples: delete_partitions_and_data(\"my_database\", \"my_table\", \"year = 2020 and month = 5\")</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef delete_partitions_and_data(\n    database: str, table: str, expression: str, boto3_session=None\n):\n    \"\"\"\n    Deletes partitions and the underlying data on S3 from an Athena\n    database table matching an expression.\n\n    Args:\n        database (str): The database name.\n        table (str): The table name.\n        expression (str): The expression to match.\n\n    Please see\n    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/glue.html#Glue.Client.get_partitions # noqa\n    for instructions on the expression construction, but at a basic level\n    you can use SQL syntax on your partition columns.\n\n    Examples:\n    delete_partitions_and_data(\"my_database\", \"my_table\", \"year = 2020 and month = 5\")\n    \"\"\"\n\n    matched_partitions = wr.catalog.get_partitions(\n        database, table, expression=expression, boto3_session=boto3_session\n    )\n    # Delete data at partition locations\n    for location in matched_partitions:\n        wr.s3.delete_objects(location, boto3_session=boto3_session)\n    # Delete partitions\n    wr.catalog.delete_partitions(\n        table,\n        database,\n        list(matched_partitions.values()),\n        boto3_session=boto3_session,\n    )\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.save_query_to_parquet","title":"<code>save_query_to_parquet(sql, file_path)</code>","text":"<p>Saves the results of a query to a parquet file at a given location.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL query.</p> required <code>file_path</code> <code>str</code> <p>The path to save the result to.</p> required <p>Examples:</p> <p>save_query_to_parquet(     \"select * from my database.my_table\",     \"result.parquet\" )</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def save_query_to_parquet(sql: str, file_path: str) -&gt; None:\n    \"\"\"\n    Saves the results of a query to a parquet file\n    at a given location.\n\n    Args:\n        sql (str): The SQL query.\n        file_path (str): The path to save the result to.\n\n    Examples:\n    save_query_to_parquet(\n        \"select * from my database.my_table\",\n        \"result.parquet\"\n    )\n    \"\"\"\n\n    df = read_sql_query(sql)\n    df.to_parquet(file_path)\n\n    return None\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.dataframe_to_temp_table","title":"<code>dataframe_to_temp_table(df, table, boto3_session=None)</code>","text":"<p>Creates a temporary table from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame</p> required <code>table</code> <code>str</code> <p>The name of the table in the temporary database</p> required <code>boto3_session</code> <p>opeional boto3 sesssion</p> <code>None</code> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef dataframe_to_temp_table(df: pd.DataFrame, table: str, boto3_session=None) -&gt; None:\n    \"\"\"\n    Creates a temporary table from a dataframe.\n\n    Args:\n        df (pandas.DataFrame): A pandas DataFrame\n        table (str): The name of the table in the temporary database\n        boto3_session: opeional boto3 sesssion\n    \"\"\"\n    user_id, table_dir = get_user_id_and_table_dir(boto3_session=boto3_session)\n    db = get_database_name_from_userid(user_id)\n    _create_temp_database(db, boto3_session=boto3_session)\n\n    delete_temp_table(table, boto3_session=boto3_session)\n\n    # Include timestamp in path to avoid permissions problems with\n    # previous sessions\n    ts = str(time.time()).replace(\".\", \"\")\n    path = s3_path_join(table_dir, ts, table)\n    dataframe_to_table(df, db, table, path, boto3_session=boto3_session)\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.dataframe_to_table","title":"<code>dataframe_to_table(df, database, table, location, mode='overwrite', partition_cols=None, boto3_session=None, **kwargs)</code>","text":"<p>Creates a table from a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame</p> required <code>database</code> <code>str</code> <p>Database name</p> required <code>table</code> <code>str</code> <p>Table name</p> required <code>location</code> <code>str</code> <p>S3 path to where the table should be stored</p> required <code>mode</code> <code>str</code> <p>\"overwrite\" (default), \"append\", or \"overwrite_partitions\"</p> <code>'overwrite'</code> <code>partition_cols</code> <code>List[str]</code> <p>partition columns (optional)</p> <code>None</code> <code>boto3_session</code> <p>optional boto3 session</p> <code>None</code> <code>**kwargs</code> <p>arguments for to_parquet</p> <code>{}</code> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef dataframe_to_table(\n    df: pd.DataFrame,\n    database: str,\n    table: str,\n    location: str,\n    mode: str = \"overwrite\",\n    partition_cols: Optional[List[str]] = None,\n    boto3_session=None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Creates a table from a dataframe.\n\n    Args:\n        df (pandas.DataFrame): A pandas DataFrame\n        database (str): Database name\n        table (str): Table name\n        location (str): S3 path to where the table should be stored\n        mode (str): \"overwrite\" (default), \"append\", or \"overwrite_partitions\"\n        partition_cols (List[str]): partition columns (optional)\n        boto3_session: optional boto3 session\n        **kwargs: arguments for to_parquet\n    \"\"\"\n\n    # Write table\n    wr.s3.to_parquet(\n        df,\n        path=s3_path_join(location, table + \".parquet\"),\n        dataset=True,\n        database=database,\n        table=table,\n        boto3_session=boto3_session,\n        mode=mode,\n        partition_cols=partition_cols,\n        compression=\"snappy\",\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.create_database","title":"<code>create_database(database, **kwargs)</code>","text":"<p>Creates a new database.</p> <p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The name of the database</p> required <p>Returns:</p> Type Description <code>bool</code> <p>False if the database already exists, True if</p> <code>bool</code> <p>it has been created.</p> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>def create_database(database: str, **kwargs) -&gt; bool:\n    \"\"\"\n    Creates a new database.\n\n    Args:\n        database (str): The name of the database\n\n    Returns:\n        False if the database already exists, True if\n        it has been created.\n    \"\"\"\n\n    if database in (db[\"Name\"] for db in wr.catalog.get_databases()):\n        return False\n    wr.catalog.create_database(database, **kwargs)\n    return True\n</code></pre>"},{"location":"api/wrangler/#pydbtools._wrangler.file_to_table","title":"<code>file_to_table(path, database, table, location, mode='overwrite', partition_cols=None, boto3_session=None, chunksize=None, metadata=None, **kwargs)</code>","text":"<p>Writes a csv, json, or parquet file to a database table.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The location of the file</p> required <code>database</code> <code>str</code> <p>database name</p> required <code>table</code> <code>str</code> <p>table name</p> required <code>location</code> <code>str</code> <p>s3 file path to table</p> required <code>mode</code> <code>str</code> <p>\"overwrite\" (default), \"append\", \"overwrite_partitions\"</p> <code>'overwrite'</code> <code>partition_cols</code> <code>List[str]</code> <p>partition columns (optional)</p> <code>None</code> <code>boto3_session</code> <p>optional boto3 session</p> <code>None</code> <code>chunksize</code> <code>Union[int, str]</code> <p>size of chunks in memory or rows, e.g. \"100MB\", 100000</p> <code>None</code> <code>metadata</code> <p>mojap_metadata instance</p> <code>None</code> <code>**kwargs</code> <p>arguments for arrow_pd_parser.reader.read e.g. use chunksize for very large files, metadata to apply metadata</p> <code>{}</code> Source code in <code>pydbtools/_wrangler.py</code> <pre><code>@init_athena_params(allow_boto3_session=True)\ndef file_to_table(\n    path: str,\n    database: str,\n    table: str,\n    location: str,\n    mode: str = \"overwrite\",\n    partition_cols: Optional[List[str]] = None,\n    boto3_session=None,\n    chunksize=None,\n    metadata=None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"\n    Writes a csv, json, or parquet file to a database table.\n\n    Args:\n        path (str): The location of the file\n        database (str): database name\n        table (str): table name\n        location (str): s3 file path to table\n        mode (str): \"overwrite\" (default), \"append\", \"overwrite_partitions\"\n        partition_cols (List[str]): partition columns (optional)\n        boto3_session: optional boto3 session\n        chunksize Union[int,str]: size of chunks in memory or rows,\n            e.g. \"100MB\", 100000\n        metadata: mojap_metadata instance\n        **kwargs: arguments for arrow_pd_parser.reader.read\n            e.g. use chunksize for very large files, metadata\n            to apply metadata\n    \"\"\"\n\n    dfs = reader.read(path, chunksize=chunksize, metadata=metadata, **kwargs)\n    if isinstance(dfs, pd.DataFrame):\n        # Convert single dataframe to iterator\n        dfs = iter([dfs])\n    elif mode == \"overwrite_partitions\":\n        raise ValueError(\n            \"overwrite_partitions and a set chunksize \"\n            + \"can't be used at the same time\"\n        )\n\n    for df in dfs:\n        dataframe_to_table(\n            df,\n            database,\n            table,\n            location,\n            partition_cols=partition_cols,\n            mode=mode,\n            boto3_session=boto3_session,\n        )\n        mode = \"append\"\n</code></pre>"},{"location":"examples/create_temporary_tables/","title":"Create Temporary Tables","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\ndb_name = f\"aws_example_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"{db_name} found deleting\")\n    wr.catalog.delete_database(name=db_name)\n\n# Setup source database\n# Create the database\nwr.catalog.create_database(db_name)\n\n# Iterate through the tables in data/ and write them to our db using awswrangler\nfor table_name in [\"department\", \"employees\", \"sales\"]:\n\n    df = pd.read_csv(f\"data/{table_name}.csv\")\n    table_path = os.path.join(source_db_base_path, table_name)\n    wr.s3.to_parquet(\n        df=df,\n        path=table_path,\n        index=False,\n        dataset=True,  # True allows the other params below i.e. overwriting to db.table\n        database=db_name,\n        table=table_name,\n        mode=\"overwrite\",\n    )\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  db_name = f\"aws_example_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     print(f\"{db_name} found deleting\")     wr.catalog.delete_database(name=db_name)  # Setup source database # Create the database wr.catalog.create_database(db_name)  # Iterate through the tables in data/ and write them to our db using awswrangler for table_name in [\"department\", \"employees\", \"sales\"]:      df = pd.read_csv(f\"data/{table_name}.csv\")     table_path = os.path.join(source_db_base_path, table_name)     wr.s3.to_parquet(         df=df,         path=table_path,         index=False,         dataset=True,  # True allows the other params below i.e. overwriting to db.table         database=db_name,         table=table_name,         mode=\"overwrite\",     ) In\u00a0[4]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {db_name}.employees LIMIT 5\", ctas_approach=False\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {db_name}.employees LIMIT 5\", ctas_approach=False ) Out[4]: employee_id sex forename surname department_id manager_id 0 1 M Dexter Mitchell 1.0 17.0 1 2 F Summer Bennett 1.0 17.0 2 3 M Pip Carter 1.0 17.0 3 4 F Bella Long 1.0 17.0 4 5 F Lexie Perry NaN 17.0 In\u00a0[5]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {db_name}.department LIMIT 5\", ctas_approach=False\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {db_name}.department LIMIT 5\", ctas_approach=False ) Out[5]: department_id department_name 0 1 Sales 1 2 Admin 2 3 Management 3 4 Technical 4 5 Maintenance In\u00a0[6]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {db_name}.sales LIMIT 5\", ctas_approach=False\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {db_name}.sales LIMIT 5\", ctas_approach=False ) Out[6]: employee_id qtr sales 0 1 1 768.17 1 2 1 391.98 2 3 1 406.36 3 4 1 816.25 4 5 1 437.05 <p>pydbtools has a create temp table function that allows you to create tables which you can refer to in a <code>__temp__</code> database.</p> <p>First create a total_sales table:</p> In\u00a0[7]: Copied! <pre>sql = f\"\"\"\nSELECT employee_id, sum(sales) as total_sales\nFROM {db_name}.sales\nGROUP BY employee_id\n\"\"\"\nprint(sql)\npydb.create_temp_table(sql, table_name=\"total_sales\")\n</pre> sql = f\"\"\" SELECT employee_id, sum(sales) as total_sales FROM {db_name}.sales GROUP BY employee_id \"\"\" print(sql) pydb.create_temp_table(sql, table_name=\"total_sales\") <pre>\nSELECT employee_id, sum(sales) as total_sales\nFROM aws_example_mratford.sales\nGROUP BY employee_id\n\n</pre> <p>Then create a table of employee names from the sales department:</p> In\u00a0[8]: Copied! <pre>sql = f\"\"\"\nSELECT e.employee_id, e.forename, e.surname, d.department_name\nFROM {db_name}.employees AS e\nLEFT JOIN {db_name}.department AS d\nON e.department_id = d.department_id\nWHERE e.department_id = 1\n\"\"\"\nprint(sql)\npydb.create_temp_table(sql, table_name=\"sales_employees\")\n</pre> sql = f\"\"\" SELECT e.employee_id, e.forename, e.surname, d.department_name FROM {db_name}.employees AS e LEFT JOIN {db_name}.department AS d ON e.department_id = d.department_id WHERE e.department_id = 1 \"\"\" print(sql) pydb.create_temp_table(sql, table_name=\"sales_employees\") <pre>\nSELECT e.employee_id, e.forename, e.surname, d.department_name\nFROM aws_example_mratford.employees AS e\nLEFT JOIN aws_example_mratford.department AS d\nON e.department_id = d.department_id\nWHERE e.department_id = 1\n\n</pre> <p>Finally return our final table</p> In\u00a0[9]: Copied! <pre>sql = f\"\"\"\nSELECT se.*, ts.total_sales\nFROM __temp__.sales_employees AS se\nINNER JOIN __temp__.total_sales AS ts\nON se.employee_id = ts.employee_id\n\"\"\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False).head(10)\n</pre> sql = f\"\"\" SELECT se.*, ts.total_sales FROM __temp__.sales_employees AS se INNER JOIN __temp__.total_sales AS ts ON se.employee_id = ts.employee_id \"\"\" print(sql) pydb.read_sql_query(sql, ctas_approach=False).head(10) <pre>\nSELECT se.*, ts.total_sales\nFROM __temp__.sales_employees AS se\nINNER JOIN __temp__.total_sales AS ts\nON se.employee_id = ts.employee_id\n\n</pre> Out[9]: employee_id forename surname department_name total_sales 0 1 Dexter Mitchell Sales 2911.65 1 2 Summer Bennett Sales 1785.73 2 3 Pip Carter Sales 2590.60 3 4 Bella Long Sales 2996.54 4 6 Robert Roberts Sales 2207.77 5 7 Iris Alexander Sales 2465.13 6 9 Evan Carter Sales 2279.84 7 10 Lauren Powell Sales 1935.67 8 11 Alice James Sales 3092.89 9 12 Owen Scott Sales 2286.28 In\u00a0[10]: Copied! <pre>df = pd.read_csv(\"data/sales.csv\")\npydb.dataframe_to_temp_table(df, \"sales\")\npydb.read_sql_query(\n    \"select qtr, sum(sales) as sales from __temp__.sales group by qtr\"\n)\n</pre> df = pd.read_csv(\"data/sales.csv\") pydb.dataframe_to_temp_table(df, \"sales\") pydb.read_sql_query(     \"select qtr, sum(sales) as sales from __temp__.sales group by qtr\" ) Out[10]: qtr sales 0 4 27558.68 1 2 30696.60 2 1 28167.78 3 3 26419.31 In\u00a0[11]: Copied! <pre>### Clean up\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"{db_name} found deleting\")\n    wr.catalog.delete_database(name=db_name)\n</pre> ### Clean up  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     print(f\"{db_name} found deleting\")     wr.catalog.delete_database(name=db_name) <pre>deleting objs\naws_example_mratford found deleting\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/create_temporary_tables/#create-temporary-tables","title":"Create Temporary Tables\u00b6","text":"<p>This tutorial demonstrates how to create tempoary tables in athena using pydbtools</p>"},{"location":"examples/create_temporary_tables/#setup","title":"Setup\u00b6","text":"<p>Just run this script to create the source database so we can use it for our example.</p>"},{"location":"examples/create_temporary_tables/#task","title":"Task\u00b6","text":"<p>We are going to create a table that shows total sales per employee using all 3 tables</p>"},{"location":"examples/create_temporary_tables/#creating-a-temporary-table-from-a-dataframe","title":"Creating a temporary table from a dataframe\u00b6","text":"<p>You can also use an existing dataframe as a table in the temporary database and run SQL queries on it.</p>"},{"location":"examples/create_temporary_tables_from_sql_file/","title":"Create Temporary Tables Using SQL Files","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\ndb_name = f\"aws_example_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\npydb.delete_database_and_data(db_name)\n\n# Setup source database\n# Create the database\npydb.create_database(db_name)\n\n# Iterate through the tables in data/ and write them to our db using awswrangler\nfor table_name in [\"department\", \"employees\", \"sales\"]:\n    table_path = pydb.s3_path_join(source_db_base_path, f\"{table_name}/\")\n    pydb.file_to_table(\n        path=f\"data/{table_name}.csv\",\n        database=db_name,\n        table=table_name,\n        location=table_path,\n    )\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  db_name = f\"aws_example_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  pydb.delete_database_and_data(db_name)  # Setup source database # Create the database pydb.create_database(db_name)  # Iterate through the tables in data/ and write them to our db using awswrangler for table_name in [\"department\", \"employees\", \"sales\"]:     table_path = pydb.s3_path_join(source_db_base_path, f\"{table_name}/\")     pydb.file_to_table(         path=f\"data/{table_name}.csv\",         database=db_name,         table=table_name,         location=table_path,     ) In\u00a0[4]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {db_name}.employees LIMIT 5\", ctas_approach=False\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {db_name}.employees LIMIT 5\", ctas_approach=False ) Out[4]: employee_id sex forename surname department_id manager_id 0 1 M Dexter Mitchell 1 17 1 2 F Summer Bennett 1 17 2 3 M Pip Carter 1 17 3 4 F Bella Long 1 17 4 5 F Lexie Perry &lt;NA&gt; 17 In\u00a0[5]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {db_name}.department LIMIT 5\", ctas_approach=False\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {db_name}.department LIMIT 5\", ctas_approach=False ) Out[5]: department_id department_name 0 1 Sales 1 2 Admin 2 3 Management 3 4 Technical 4 5 Maintenance In\u00a0[6]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {db_name}.sales LIMIT 5\", ctas_approach=False\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {db_name}.sales LIMIT 5\", ctas_approach=False ) Out[6]: employee_id qtr sales 0 1 1 768.17 1 2 1 391.98 2 3 1 406.36 3 4 1 816.25 4 5 1 437.05 <p>pydbtools has <code>read_sql_queries</code> and <code>read_sql_queries</code> functions that allow you to create temporary tables within SQL which you can refer to in a <code>__temp__</code> database.</p> <p>First create a total_sales table:</p> In\u00a0[7]: Copied! <pre>sql = f\"\"\"\nCREATE TEMP TABLE total_sales AS\nSELECT employee_id, sum(sales) as total_sales\nFROM {db_name}.sales\nGROUP BY employee_id;\n\"\"\"\nprint(sql)\n</pre> sql = f\"\"\" CREATE TEMP TABLE total_sales AS SELECT employee_id, sum(sales) as total_sales FROM {db_name}.sales GROUP BY employee_id; \"\"\" print(sql) <pre>\nCREATE TEMP TABLE total_sales AS\nSELECT employee_id, sum(sales) as total_sales\nFROM aws_example_mratford.sales\nGROUP BY employee_id;\n\n</pre> <p>Then create a table of employee names from the sales department:</p> In\u00a0[8]: Copied! <pre>sql += f\"\"\"\nCREATE TEMP TABLE sales_employees AS\nSELECT e.employee_id, e.forename, e.surname, d.department_name\nFROM {db_name}.employees AS e\nLEFT JOIN {db_name}.department AS d\nON e.department_id = d.department_id\nWHERE e.department_id = 1;\n\"\"\"\nprint(sql)\n</pre> sql += f\"\"\" CREATE TEMP TABLE sales_employees AS SELECT e.employee_id, e.forename, e.surname, d.department_name FROM {db_name}.employees AS e LEFT JOIN {db_name}.department AS d ON e.department_id = d.department_id WHERE e.department_id = 1; \"\"\" print(sql) <pre>\nCREATE TEMP TABLE total_sales AS\nSELECT employee_id, sum(sales) as total_sales\nFROM aws_example_mratford.sales\nGROUP BY employee_id;\n\nCREATE TEMP TABLE sales_employees AS\nSELECT e.employee_id, e.forename, e.surname, d.department_name\nFROM aws_example_mratford.employees AS e\nLEFT JOIN aws_example_mratford.department AS d\nON e.department_id = d.department_id\nWHERE e.department_id = 1;\n\n</pre> <p>Finally return our final tables</p> <p>Note that more than one select statement can be used so the function returns an iterator yielding the results of each select.</p> In\u00a0[9]: Copied! <pre>sql += f\"\"\"\nSELECT se.*, ts.total_sales\nFROM __temp__.sales_employees AS se\nINNER JOIN __temp__.total_sales AS ts\nON se.employee_id = ts.employee_id;\n\"\"\"\nprint(sql)\n</pre> sql += f\"\"\" SELECT se.*, ts.total_sales FROM __temp__.sales_employees AS se INNER JOIN __temp__.total_sales AS ts ON se.employee_id = ts.employee_id; \"\"\" print(sql) <pre>\nCREATE TEMP TABLE total_sales AS\nSELECT employee_id, sum(sales) as total_sales\nFROM aws_example_mratford.sales\nGROUP BY employee_id;\n\nCREATE TEMP TABLE sales_employees AS\nSELECT e.employee_id, e.forename, e.surname, d.department_name\nFROM aws_example_mratford.employees AS e\nLEFT JOIN aws_example_mratford.department AS d\nON e.department_id = d.department_id\nWHERE e.department_id = 1;\n\nSELECT se.*, ts.total_sales\nFROM __temp__.sales_employees AS se\nINNER JOIN __temp__.total_sales AS ts\nON se.employee_id = ts.employee_id;\n\n</pre> In\u00a0[10]: Copied! <pre>total_sales = pydb.read_sql_queries(sql)\n</pre> total_sales = pydb.read_sql_queries(sql) In\u00a0[11]: Copied! <pre>total_sales\n</pre> total_sales Out[11]: employee_id forename surname department_name total_sales 0 1 Dexter Mitchell Sales 2911.65 1 2 Summer Bennett Sales 1785.73 2 3 Pip Carter Sales 2590.60 3 4 Bella Long Sales 2996.54 4 6 Robert Roberts Sales 2207.77 5 7 Iris Alexander Sales 2465.13 6 9 Evan Carter Sales 2279.84 7 10 Lauren Powell Sales 1935.67 8 11 Alice James Sales 3092.89 9 12 Owen Scott Sales 2286.28 10 13 Sarah Patterson Sales 2711.01 11 15 Evie Morgan Sales 2613.67 12 16 Zachary Rodriguez Sales 2373.06 13 17 Madison Kelly Sales 2302.02 14 18 Jake Harris Sales 1759.39 15 19 Emma Cooper Sales 2442.86 16 20 Eva Peterson Sales 2851.36 17 21 Isaac White Sales 1643.14 18 23 Sienna James Sales 3036.47 19 24 Seren Diaz Sales 2248.35 20 26 Angel Hayes Sales 1994.65 21 27 Evan Carter Sales 3050.02 22 28 Olivia Rogers Sales 2071.77 23 29 Molly Gray Sales 2478.25 24 30 Lucas Moore Sales 2251.47 25 31 Kitty Russell Sales 2461.53 26 32 Leo Thomas Sales 2693.30 27 33 Evie Morgan Sales 2685.07 28 34 Eli Mitchell Sales 1387.50 29 36 joseph Martin Sales 1580.06 30 37 Alexandra Russell Sales 1984.24 31 38 Keira Hughes Sales 2158.55 32 39 Sophie Morris Sales 1931.27 33 40 Tobias Collins Sales 2595.53 34 41 Lola Watson Sales 1326.88 35 42 Maisie Torres Sales 1688.76 36 43 Phoebe Peterson Sales 2073.25 37 44 Mia Murphy Sales 2184.14 38 45 Freya Bailey Sales 2778.84 39 46 Sebastian Hall Sales 2547.63 40 47 Emily Stewart Sales 1489.52 <p>The <code>read_sql_queries_gen</code> function allows you to use more than <code>SELECT</code> statement, returning an iterator of dataframes.</p> In\u00a0[12]: Copied! <pre>sql += f\"\"\"\nSELECT forename, surname, sum(s.sales) as q1_sales\nFROM __temp__.sales_employees AS se\nLEFT JOIN {db_name}.sales AS s\nON se.employee_id = s.employee_id\nGROUP BY forename, surname;\n\"\"\"\nprint(sql)\n</pre> sql += f\"\"\" SELECT forename, surname, sum(s.sales) as q1_sales FROM __temp__.sales_employees AS se LEFT JOIN {db_name}.sales AS s ON se.employee_id = s.employee_id GROUP BY forename, surname; \"\"\" print(sql) <pre>\nCREATE TEMP TABLE total_sales AS\nSELECT employee_id, sum(sales) as total_sales\nFROM aws_example_mratford.sales\nGROUP BY employee_id;\n\nCREATE TEMP TABLE sales_employees AS\nSELECT e.employee_id, e.forename, e.surname, d.department_name\nFROM aws_example_mratford.employees AS e\nLEFT JOIN aws_example_mratford.department AS d\nON e.department_id = d.department_id\nWHERE e.department_id = 1;\n\nSELECT se.*, ts.total_sales\nFROM __temp__.sales_employees AS se\nINNER JOIN __temp__.total_sales AS ts\nON se.employee_id = ts.employee_id;\n\nSELECT forename, surname, sum(s.sales) as q1_sales\nFROM __temp__.sales_employees AS se\nLEFT JOIN aws_example_mratford.sales AS s\nON se.employee_id = s.employee_id\nGROUP BY forename, surname;\n\n</pre> In\u00a0[13]: Copied! <pre>total_sales, q1_sales = tuple(pydb.read_sql_queries_gen(sql))\nq1_sales\n</pre> total_sales, q1_sales = tuple(pydb.read_sql_queries_gen(sql)) q1_sales Out[13]: forename surname q1_sales 0 Dexter Mitchell 2911.65 1 Elliot King NaN 2 Madison Kelly 2302.02 3 Eva Peterson 2851.36 4 Olivia Rogers 2071.77 5 Angel Hayes 1994.65 6 Bella Long 2996.54 7 Rosie Bennett NaN 8 Lauren Powell 1935.67 9 Tobias Collins 2595.53 10 Evan Carter 5329.86 11 Sarah Patterson 2711.01 12 Robert Roberts 2207.77 13 Emily Stewart 1489.52 14 Owen Scott 2286.28 15 joseph Martin 1580.06 16 Keira Hughes 2158.55 17 Iris Alexander 2465.13 18 Eli Mitchell 1387.50 19 Pip Carter 2590.60 20 Kitty Russell 2461.53 21 Lola Watson 1326.88 22 Lucas Moore 2251.47 23 Sebastian Hall 2547.63 24 Phoebe Peterson 2073.25 25 Zachary Rodriguez 2373.06 26 Emma Cooper 2442.86 27 Seren Diaz 2248.35 28 Sophie Morris 1931.27 29 Alexandra Russell 1984.24 30 Summer Bennett 1785.73 31 Jake Harris 1759.39 32 Alice James 3092.89 33 Mia Murphy 2184.14 34 Freya Bailey 2778.84 35 Maisie Torres 1688.76 36 Leo Thomas 2693.30 37 Evie Morgan 5298.74 38 Isaac White 1643.14 39 Molly Gray 2478.25 40 Sienna James 3036.47 In\u00a0[14]: Copied! <pre>### Clean up\n\npydb.delete_database_and_data(db_name)\n</pre> ### Clean up  pydb.delete_database_and_data(db_name) Out[14]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/create_temporary_tables_from_sql_file/#create-temporary-tables-using-sql-files","title":"Create Temporary Tables Using SQL Files\u00b6","text":"<p>This tutorial demonstrates how to create temporary tables in athena using <code>pydbtools.read_sql_queries</code>. This is an amended version of create_temporary_version.ipynb.</p>"},{"location":"examples/create_temporary_tables_from_sql_file/#setup","title":"Setup\u00b6","text":"<p>Just run this script to create the source database so we can use it for our example.</p>"},{"location":"examples/create_temporary_tables_from_sql_file/#task","title":"Task\u00b6","text":"<p>We are going to create a table that shows total sales per employee using all 3 tables.</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/","title":"Creating and Maintaining Database Tables in Athena","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\nsource_db_name = f\"source_db_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif source_db_name in df_dbs[\"Database\"].to_list():\n    print(f\"{source_db_name} found deleting\")\n    wr.catalog.delete_database(name=source_db_name)\n\n# Setup source database\n# Create the database\nwr.catalog.create_database(source_db_name)\n\n# Iterate through the tables in data/ and write them to our db using awswrangler\nfor table_name in [\"department\", \"employees\", \"sales\"]:\n\n    df = pd.read_csv(f\"data/{table_name}.csv\")\n    table_path = os.path.join(source_db_base_path, f\"{table_name}/\")\n    wr.s3.to_parquet(\n        df=df,\n        path=table_path,\n        index=False,\n        dataset=True,  # True allows the other params below i.e. overwriting to db.table\n        database=source_db_name,\n        table=table_name,\n        mode=\"overwrite\",\n    )\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  source_db_name = f\"source_db_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if source_db_name in df_dbs[\"Database\"].to_list():     print(f\"{source_db_name} found deleting\")     wr.catalog.delete_database(name=source_db_name)  # Setup source database # Create the database wr.catalog.create_database(source_db_name)  # Iterate through the tables in data/ and write them to our db using awswrangler for table_name in [\"department\", \"employees\", \"sales\"]:      df = pd.read_csv(f\"data/{table_name}.csv\")     table_path = os.path.join(source_db_base_path, f\"{table_name}/\")     wr.s3.to_parquet(         df=df,         path=table_path,         index=False,         dataset=True,  # True allows the other params below i.e. overwriting to db.table         database=source_db_name,         table=table_name,         mode=\"overwrite\",     ) <pre>deleting objs\nsource_db_mratford found deleting\n</pre> In\u00a0[4]: Copied! <pre>new_db_name = f\"new_db_{foldername}\"\nnew_db_base_path = f\"s3://{bucketname}/{foldername}/new_db/\"\n\nsql = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {new_db_name}\nCOMMENT 'example or running queries and insert to'\nLOCATION '{new_db_base_path}'\n\"\"\"\nprint(sql)\n_ = pydb.start_query_execution_and_wait(sql)\n</pre> new_db_name = f\"new_db_{foldername}\" new_db_base_path = f\"s3://{bucketname}/{foldername}/new_db/\"  sql = f\"\"\" CREATE DATABASE IF NOT EXISTS {new_db_name} COMMENT 'example or running queries and insert to' LOCATION '{new_db_base_path}' \"\"\" print(sql) _ = pydb.start_query_execution_and_wait(sql) <pre>\nCREATE DATABASE IF NOT EXISTS new_db_mratford\nCOMMENT 'example or running queries and insert to'\nLOCATION 's3://alpha-everyone/mratford/new_db/'\n\n</pre> In\u00a0[5]: Copied! <pre># Note our table s3 path is saved in the following format:\n# s3://&lt;bucket&gt;/&lt;path to database folder&gt;/&lt;table_name&gt;/\n# You don't have to do this but it is strongly recommended to make it easier\n# to map your schemas to your data.\n\nsales_report_s3_path = os.path.join(new_db_base_path, \"sales_report/\")\n\nsql = f\"\"\"\nCREATE TABLE {new_db_name}.sales_report WITH\n(\n    external_location='{sales_report_s3_path}'\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM {source_db_name}.sales\nWHERE qtr IN (1,2)\nGROUP BY qtr\n\"\"\"\nprint(sql)\n_ = pydb.start_query_execution_and_wait(sql)\n</pre> # Note our table s3 path is saved in the following format: # s3:///// # You don't have to do this but it is strongly recommended to make it easier # to map your schemas to your data.  sales_report_s3_path = os.path.join(new_db_base_path, \"sales_report/\")  sql = f\"\"\" CREATE TABLE {new_db_name}.sales_report WITH (     external_location='{sales_report_s3_path}' ) AS SELECT qtr as sales_quarter, sum(sales) AS total_sales FROM {source_db_name}.sales WHERE qtr IN (1,2) GROUP BY qtr \"\"\" print(sql) _ = pydb.start_query_execution_and_wait(sql) <pre>\nCREATE TABLE new_db_mratford.sales_report WITH\n(\n    external_location='s3://alpha-everyone/mratford/new_db/sales_report/'\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM source_db_mratford.sales\nWHERE qtr IN (1,2)\nGROUP BY qtr\n\n</pre> In\u00a0[6]: Copied! <pre># Let's say we want to add more data into our table\n\nsql = f\"\"\"\nINSERT INTO {new_db_name}.sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM {source_db_name}.sales\nWHERE qtr IN (3,4)\nGROUP BY qtr\n\"\"\"\nprint(sql)\n_ = pydb.start_query_execution_and_wait(sql)\n</pre> # Let's say we want to add more data into our table  sql = f\"\"\" INSERT INTO {new_db_name}.sales_report SELECT qtr as sales_quarter, sum(sales) AS total_sales FROM {source_db_name}.sales WHERE qtr IN (3,4) GROUP BY qtr \"\"\" print(sql) _ = pydb.start_query_execution_and_wait(sql) <pre>\nINSERT INTO new_db_mratford.sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM source_db_mratford.sales\nWHERE qtr IN (3,4)\nGROUP BY qtr\n\n</pre> In\u00a0[7]: Copied! <pre># Now lets see what our sales_report looks like\npydb.read_sql_query(\n    f\"SELECT * FROM {new_db_name}.sales_report\",\n    database=None,\n    ctas_approach=False,\n)\n</pre> # Now lets see what our sales_report looks like pydb.read_sql_query(     f\"SELECT * FROM {new_db_name}.sales_report\",     database=None,     ctas_approach=False, ) Out[7]: sales_quarter total_sales 0 1 28167.78 1 2 30696.60 2 3 26419.31 3 4 27558.68 In\u00a0[8]: Copied! <pre>sales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")\n\nsql = f\"\"\"\nCREATE TABLE {new_db_name}.daily_sales_report WITH\n(\n    external_location='{sales_report_s3_path}',\n    partitioned_by = ARRAY['report_date']\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-01' AS report_date\nFROM {source_db_name}.sales\nGROUP BY qtr, date '2021-01-01'\n\"\"\"\nprint(sql)\n_ = pydb.start_query_execution_and_wait(sql)\n</pre> sales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")  sql = f\"\"\" CREATE TABLE {new_db_name}.daily_sales_report WITH (     external_location='{sales_report_s3_path}',     partitioned_by = ARRAY['report_date'] ) AS SELECT qtr as sales_quarter, sum(sales) AS total_sales, date '2021-01-01' AS report_date FROM {source_db_name}.sales GROUP BY qtr, date '2021-01-01' \"\"\" print(sql) _ = pydb.start_query_execution_and_wait(sql) <pre>\nCREATE TABLE new_db_mratford.daily_sales_report WITH\n(\n    external_location='s3://alpha-everyone/mratford/new_db/daily_sales_report/',\n    partitioned_by = ARRAY['report_date']\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-01' AS report_date\nFROM source_db_mratford.sales\nGROUP BY qtr, date '2021-01-01'\n\n</pre> In\u00a0[9]: Copied! <pre># Then assume we run the report the next day pretending our source database is updated every day\nsales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")\n\nsql = f\"\"\"\nINSERT INTO {new_db_name}.daily_sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-02' AS report_date\nFROM {source_db_name}.sales\nGROUP BY qtr, date '2021-01-02'\n\"\"\"\nprint(sql)\n_ = pydb.start_query_execution_and_wait(sql)\n</pre> # Then assume we run the report the next day pretending our source database is updated every day sales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")  sql = f\"\"\" INSERT INTO {new_db_name}.daily_sales_report SELECT qtr as sales_quarter, sum(sales) AS total_sales, date '2021-01-02' AS report_date FROM {source_db_name}.sales GROUP BY qtr, date '2021-01-02' \"\"\" print(sql) _ = pydb.start_query_execution_and_wait(sql) <pre>\nINSERT INTO new_db_mratford.daily_sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-02' AS report_date\nFROM source_db_mratford.sales\nGROUP BY qtr, date '2021-01-02'\n\n</pre> In\u00a0[10]: Copied! <pre>pydb.read_sql_query(\n    f\"SELECT * FROM {new_db_name}.daily_sales_report\",\n    database=None,\n    ctas_approach=False,\n)\n</pre> pydb.read_sql_query(     f\"SELECT * FROM {new_db_name}.daily_sales_report\",     database=None,     ctas_approach=False, ) Out[10]: sales_quarter total_sales report_date 0 4 27558.68 2021-01-01 1 2 30696.60 2021-01-01 2 1 28167.78 2021-01-01 3 3 26419.31 2021-01-01 4 2 30696.60 2021-01-02 5 3 26419.31 2021-01-02 6 1 28167.78 2021-01-02 7 4 27558.68 2021-01-02 In\u00a0[11]: Copied! <pre># Clean up\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nfor db_name in [source_db_name, new_db_name]:\n    if db_name in df_dbs[\"Database\"].to_list():\n        print(f\"{db_name} found deleting\")\n        wr.catalog.delete_database(name=db_name)\n</pre> # Clean up # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) for db_name in [source_db_name, new_db_name]:     if db_name in df_dbs[\"Database\"].to_list():         print(f\"{db_name} found deleting\")         wr.catalog.delete_database(name=db_name) <pre>deleting objs\nsource_db_mratford found deleting\nnew_db_mratford found deleting\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#creating-and-maintaining-database-tables-in-athena","title":"Creating and Maintaining Database Tables in Athena\u00b6","text":"<p>In this tutorial we are going to use Athena SQL queries (via pydbtools) to create a new database from and existing databases in Athena.</p> <p>First we need to create a database of tables to act as our existing database. But then we will create a new database that holds tables that are derived from the original.</p> <p>Our source database will have the test data <code>employees.csv</code>, <code>sales.csv</code> and <code>department.csv</code> (all in the <code>data/</code> folder)</p> <p>Useful links:</p> <ul> <li>https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html</li> </ul>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#setup","title":"Setup\u00b6","text":"<p>Just run this script to create the source database so we can use it for our example.</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#now-for-the-actual-tutorial","title":"Now for the actual tutorial\u00b6","text":"<p>We are going to run all of our queries using SQL. You may have seen that I've used awswrangler to create the database above (which is fine to do). However this part of the tutorial will all be in SQL so you can run this from anything (i.e. R, Athena workbench, anything that can send queries to Athena).</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#step-1-create-the-new-database","title":"Step 1. create the new database\u00b6","text":"<p>We are going to create a new database which will generate derived tables from our source database. For ease we are going to create the database location in the same parent folder as our source database. However, in reality you probably want to create your own bucket for the database and tables to sit in so that you can control who has access to your database.</p> <p>Note: We use a lot of f-strings here to parameterise our SQL queries so for ease and understanding we are going to print out each SQL query each time just so you can see what you are actually running on athena.</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#step-2-run-a-ctas-query-to-create-your-new-derived-table-in-your-new-database","title":"Step 2. Run a CTAS query to create your new derived table in your new database\u00b6","text":"<p>We use a CTAS query as it both generates the output data into S3 but also creates the schema of the new table</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#step-3-use-an-insert-into-query-to-add-new-data-into-that-table","title":"Step 3. Use An insert into query to add new data into that table\u00b6","text":"<p>We use an Insert INTO query here as we already created the schema in the previous CTAS query</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena/#step-4-create-another-new-table-and-insert-new-data-into-it-this-time-using-partitions","title":"Step 4. Create another new table and insert new data into it this time using partitions\u00b6","text":"<p>We are going to do the same but this time partition the data and write new data into a new partition. Let's parition the data based on something like when the report was ran.</p> <p>Note: that columns that are partitions should always be the last columns in your table.</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/","title":"Creating and Maintaining Database Tables in Athena from SQL files","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\nsource_db_name = f\"source_db_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\npydb.delete_database_and_data(source_db_name)\n\n# Setup source database\n# Create the database\npydb.create_database(source_db_name)\n\n# Iterate through the tables in data/ and write them to our db using file_to_table\nfor table_name in [\"department\", \"employees\", \"sales\"]:\n    table_path = pydb.s3_path_join(source_db_base_path, f\"{table_name}/\")\n    pydb.file_to_table(\n        path=f\"data/{table_name}.csv\",\n        database=source_db_name,\n        table=table_name,\n        location=table_path,\n    )\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  source_db_name = f\"source_db_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  pydb.delete_database_and_data(source_db_name)  # Setup source database # Create the database pydb.create_database(source_db_name)  # Iterate through the tables in data/ and write them to our db using file_to_table for table_name in [\"department\", \"employees\", \"sales\"]:     table_path = pydb.s3_path_join(source_db_base_path, f\"{table_name}/\")     pydb.file_to_table(         path=f\"data/{table_name}.csv\",         database=source_db_name,         table=table_name,         location=table_path,     ) In\u00a0[4]: Copied! <pre>new_db_name = f\"new_db_{foldername}\"\nnew_db_base_path = f\"s3://{bucketname}/{foldername}/new_db/\"\n\nsql = f\"\"\"\nCREATE DATABASE IF NOT EXISTS {new_db_name}\nCOMMENT 'example or running queries and insert to'\nLOCATION '{new_db_base_path}';\n\"\"\"\n</pre> new_db_name = f\"new_db_{foldername}\" new_db_base_path = f\"s3://{bucketname}/{foldername}/new_db/\"  sql = f\"\"\" CREATE DATABASE IF NOT EXISTS {new_db_name} COMMENT 'example or running queries and insert to' LOCATION '{new_db_base_path}'; \"\"\" In\u00a0[5]: Copied! <pre># Note our table s3 path is saved in the following format:\n# s3://&lt;bucket&gt;/&lt;path to database folder&gt;/&lt;table_name&gt;/\n# You don't have to do this but it is strongly recommended to make it easier\n# to map your schemas to your data.\n\nsales_report_s3_path = os.path.join(new_db_base_path, \"sales_report/\")\n\nsql += f\"\"\"\nCREATE TABLE {new_db_name}.sales_report WITH\n(\n    external_location='{sales_report_s3_path}'\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM {source_db_name}.sales\nWHERE qtr IN (1,2)\nGROUP BY qtr;\n\"\"\"\n</pre> # Note our table s3 path is saved in the following format: # s3:///// # You don't have to do this but it is strongly recommended to make it easier # to map your schemas to your data.  sales_report_s3_path = os.path.join(new_db_base_path, \"sales_report/\")  sql += f\"\"\" CREATE TABLE {new_db_name}.sales_report WITH (     external_location='{sales_report_s3_path}' ) AS SELECT qtr as sales_quarter, sum(sales) AS total_sales FROM {source_db_name}.sales WHERE qtr IN (1,2) GROUP BY qtr; \"\"\" In\u00a0[6]: Copied! <pre># Let's say we want to add more data into our table\n\nsql += f\"\"\"\nINSERT INTO {new_db_name}.sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM {source_db_name}.sales\nWHERE qtr IN (3,4)\nGROUP BY qtr;\n\"\"\"\n</pre> # Let's say we want to add more data into our table  sql += f\"\"\" INSERT INTO {new_db_name}.sales_report SELECT qtr as sales_quarter, sum(sales) AS total_sales FROM {source_db_name}.sales WHERE qtr IN (3,4) GROUP BY qtr; \"\"\" <p>Add SQL to create a quarterly sales report.</p> In\u00a0[7]: Copied! <pre>sql += f\"\"\"\nSELECT * FROM {new_db_name}.sales_report;\n\"\"\"\n</pre> sql += f\"\"\" SELECT * FROM {new_db_name}.sales_report; \"\"\" In\u00a0[8]: Copied! <pre>sales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")\n\nsql += f\"\"\"\nCREATE TABLE {new_db_name}.daily_sales_report WITH\n(\n    external_location='{sales_report_s3_path}',\n    partitioned_by = ARRAY['report_date']\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-01' AS report_date\nFROM {source_db_name}.sales\nGROUP BY qtr, date '2021-01-01';\n\"\"\"\n</pre> sales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")  sql += f\"\"\" CREATE TABLE {new_db_name}.daily_sales_report WITH (     external_location='{sales_report_s3_path}',     partitioned_by = ARRAY['report_date'] ) AS SELECT qtr as sales_quarter, sum(sales) AS total_sales, date '2021-01-01' AS report_date FROM {source_db_name}.sales GROUP BY qtr, date '2021-01-01'; \"\"\" In\u00a0[9]: Copied! <pre># Then assume we run the report the next day pretending our source database is updated every day\nsales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")\n\nsql += f\"\"\"\nINSERT INTO {new_db_name}.daily_sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-02' AS report_date\nFROM {source_db_name}.sales\nGROUP BY qtr, date '2021-01-02';\n\"\"\"\n</pre> # Then assume we run the report the next day pretending our source database is updated every day sales_report_s3_path = os.path.join(new_db_base_path, \"daily_sales_report/\")  sql += f\"\"\" INSERT INTO {new_db_name}.daily_sales_report SELECT qtr as sales_quarter, sum(sales) AS total_sales, date '2021-01-02' AS report_date FROM {source_db_name}.sales GROUP BY qtr, date '2021-01-02'; \"\"\" <p>Let's look at the entire SQL statement.</p> In\u00a0[10]: Copied! <pre>print(sql)\n</pre> print(sql) <pre>\nCREATE DATABASE IF NOT EXISTS new_db_mratford\nCOMMENT 'example or running queries and insert to'\nLOCATION 's3://alpha-everyone/mratford/new_db/';\n\nCREATE TABLE new_db_mratford.sales_report WITH\n(\n    external_location='s3://alpha-everyone/mratford/new_db/sales_report/'\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM source_db_mratford.sales\nWHERE qtr IN (1,2)\nGROUP BY qtr;\n\nINSERT INTO new_db_mratford.sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM source_db_mratford.sales\nWHERE qtr IN (3,4)\nGROUP BY qtr;\n\nSELECT * FROM new_db_mratford.sales_report;\n\nCREATE TABLE new_db_mratford.daily_sales_report WITH\n(\n    external_location='s3://alpha-everyone/mratford/new_db/daily_sales_report/',\n    partitioned_by = ARRAY['report_date']\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-01' AS report_date\nFROM source_db_mratford.sales\nGROUP BY qtr, date '2021-01-01';\n\nINSERT INTO new_db_mratford.daily_sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-02' AS report_date\nFROM source_db_mratford.sales\nGROUP BY qtr, date '2021-01-02';\n\n</pre> In\u00a0[11]: Copied! <pre>sales_report = pydb.read_sql_queries(sql)\nsales_report\n</pre> sales_report = pydb.read_sql_queries(sql) sales_report Out[11]: sales_quarter total_sales 0 3 26419.31 1 4 27558.68 2 2 30696.60 3 1 28167.78 <p>Use <code>read_sql_queries_gen</code>, which returns an iterator of pandas dataframes, for more than one <code>SELECT</code> query.</p> In\u00a0[12]: Copied! <pre>sql += f\"\"\"\nSELECT * FROM {new_db_name}.daily_sales_report;\n\"\"\"\nprint(sql)\n</pre> sql += f\"\"\" SELECT * FROM {new_db_name}.daily_sales_report; \"\"\" print(sql) <pre>\nCREATE DATABASE IF NOT EXISTS new_db_mratford\nCOMMENT 'example or running queries and insert to'\nLOCATION 's3://alpha-everyone/mratford/new_db/';\n\nCREATE TABLE new_db_mratford.sales_report WITH\n(\n    external_location='s3://alpha-everyone/mratford/new_db/sales_report/'\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM source_db_mratford.sales\nWHERE qtr IN (1,2)\nGROUP BY qtr;\n\nINSERT INTO new_db_mratford.sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales\nFROM source_db_mratford.sales\nWHERE qtr IN (3,4)\nGROUP BY qtr;\n\nSELECT * FROM new_db_mratford.sales_report;\n\nCREATE TABLE new_db_mratford.daily_sales_report WITH\n(\n    external_location='s3://alpha-everyone/mratford/new_db/daily_sales_report/',\n    partitioned_by = ARRAY['report_date']\n) AS\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-01' AS report_date\nFROM source_db_mratford.sales\nGROUP BY qtr, date '2021-01-01';\n\nINSERT INTO new_db_mratford.daily_sales_report\nSELECT qtr as sales_quarter, sum(sales) AS total_sales,\ndate '2021-01-02' AS report_date\nFROM source_db_mratford.sales\nGROUP BY qtr, date '2021-01-02';\n\nSELECT * FROM new_db_mratford.daily_sales_report;\n\n</pre> In\u00a0[13]: Copied! <pre>pydb.delete_database_and_data(new_db_name)\ndf_g = pydb.read_sql_queries_gen(sql)\nsales_report = next(df_g)\nsales_report\n</pre> pydb.delete_database_and_data(new_db_name) df_g = pydb.read_sql_queries_gen(sql) sales_report = next(df_g) sales_report Out[13]: sales_quarter total_sales 0 3 26419.31 1 4 27558.68 2 2 30696.60 3 1 28167.78 In\u00a0[14]: Copied! <pre>daily_sales_report = next(df_g)\ndaily_sales_report\n</pre> daily_sales_report = next(df_g) daily_sales_report Out[14]: sales_quarter total_sales report_date 0 1 28167.78 2021-01-02 1 3 26419.31 2021-01-02 2 4 27558.68 2021-01-02 3 2 30696.60 2021-01-02 4 2 30696.60 2021-01-01 5 1 28167.78 2021-01-01 6 3 26419.31 2021-01-01 7 4 27558.68 2021-01-01 In\u00a0[15]: Copied! <pre># Clean up\npydb.delete_database_and_data(new_db_name)\n</pre> # Clean up pydb.delete_database_and_data(new_db_name) Out[15]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#creating-and-maintaining-database-tables-in-athena-from-sql-files","title":"Creating and Maintaining Database Tables in Athena from SQL files\u00b6","text":"<p>In this tutorial we are going to use Athena SQL queries (via pydbtools) to create a new database from and existing databases in Athena.</p> <p>First we need to create a database of tables to act as our existing database. But then we will create a new database that holds tables that are derived from the original.</p> <p>Our source database will have the test data <code>employees.csv</code>, <code>sales.csv</code> and <code>department.csv</code> (all in the <code>data/</code> folder)</p> <p>Useful links:</p> <ul> <li>https://docs.aws.amazon.com/athena/latest/ug/ctas-examples.html</li> </ul>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#setup","title":"Setup\u00b6","text":"<p>Just run this script to create the source database so we can use it for our example.</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#now-for-the-actual-tutorial","title":"Now for the actual tutorial\u00b6","text":"<p>We are going to run all of our queries using SQL. You may have seen that I've used awswrangler to create the database above (which is fine to do). However this part of the tutorial will all be in SQL so you can run this from anything (i.e. R, Athena workbench, anything that can send queries to Athena).</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#step-1-create-the-new-database","title":"Step 1. create the new database\u00b6","text":"<p>We are going to create a new database which will generate derived tables from our source database. For ease we are going to create the database location in the same parent folder as our source database. However, in reality you probably want to create your own bucket for the database and tables to sit in so that you can control who has access to your database.</p> <p>Note: We use a lot of f-strings here to parameterise our SQL queries so for ease and understanding we are going to print out each SQL query each time just so you can see what you are actually running on athena.</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#step-2-run-a-ctas-query-to-create-your-new-derived-table-in-your-new-database","title":"Step 2. Run a CTAS query to create your new derived table in your new database\u00b6","text":"<p>We use a CTAS query as it both generates the output data into S3 but also creates the schema of the new table</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#step-3-use-an-insert-into-query-to-add-new-data-into-that-table","title":"Step 3. Use An insert into query to add new data into that table\u00b6","text":"<p>We use an Insert INTO query here as we already created the schema in the previous CTAS query</p>"},{"location":"examples/creating_and_maintaining_database_tables_in_athena_from_sql/#step-4-create-another-new-table-and-insert-new-data-into-it-this-time-using-partitions","title":"Step 4. Create another new table and insert new data into it this time using partitions\u00b6","text":"<p>We are going to do the same but this time partition the data and write new data into a new partition. Let's parition the data based on something like when the report was ran.</p> <p>Note: that columns that are partitions should always be the last columns in your table.</p>"},{"location":"examples/delete_databases_tables_and_partitions/","title":"Delete databases, tables and partitions and data","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb <p>Set up your testing area.</p> <p>Important: substitute your own Github username below.</p> In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\ndb_name = f\"aws_example_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"{db_name} found deleting\")\n    wr.catalog.delete_database(name=db_name)\n\n# Setup source database\n# Create the database\nwr.catalog.create_database(db_name)\n\n# Iterate through the tables in data/ and write them to our db using awswrangler\nfor table_name in [\"department\", \"employees\"]:\n    df = pd.read_csv(f\"data/{table_name}.csv\")\n    table_path = os.path.join(source_db_base_path, f\"{table_name}/\")\n    wr.s3.to_parquet(\n        df=df,\n        path=table_path,\n        index=False,\n        dataset=True,  # True allows the other params below i.e. overwriting to db.table\n        database=db_name,\n        table=table_name,\n        mode=\"overwrite\",\n    )\n\n# For the sales table partition the data by employee_id and qtr\n# (reduce the size of the table for legibility)\ndf = pd.read_csv(\"data/sales.csv\").query(\"employee_id &lt; 5\")\ntable_path = os.path.join(source_db_base_path, \"sales\")\npartition_info = wr.s3.to_parquet(\n    df=df,\n    path=table_path,\n    index=False,\n    dataset=True,\n    partition_cols=[\"employee_id\", \"qtr\"],\n    database=db_name,\n    table=\"sales\",\n    mode=\"overwrite\",\n)\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  db_name = f\"aws_example_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     print(f\"{db_name} found deleting\")     wr.catalog.delete_database(name=db_name)  # Setup source database # Create the database wr.catalog.create_database(db_name)  # Iterate through the tables in data/ and write them to our db using awswrangler for table_name in [\"department\", \"employees\"]:     df = pd.read_csv(f\"data/{table_name}.csv\")     table_path = os.path.join(source_db_base_path, f\"{table_name}/\")     wr.s3.to_parquet(         df=df,         path=table_path,         index=False,         dataset=True,  # True allows the other params below i.e. overwriting to db.table         database=db_name,         table=table_name,         mode=\"overwrite\",     )  # For the sales table partition the data by employee_id and qtr # (reduce the size of the table for legibility) df = pd.read_csv(\"data/sales.csv\").query(\"employee_id &lt; 5\") table_path = os.path.join(source_db_base_path, \"sales\") partition_info = wr.s3.to_parquet(     df=df,     path=table_path,     index=False,     dataset=True,     partition_cols=[\"employee_id\", \"qtr\"],     database=db_name,     table=\"sales\",     mode=\"overwrite\", ) <pre>deleting objs\naws_example_mratford found deleting\n</pre> In\u00a0[4]: Copied! <pre>table_info = list(wr.catalog.get_tables(database=db_name))\n[x[\"Name\"] for x in table_info]\n</pre> table_info = list(wr.catalog.get_tables(database=db_name)) [x[\"Name\"] for x in table_info] Out[4]: <pre>['department', 'employees', 'sales']</pre> <p>Show the data for the <code>department</code> table.</p> In\u00a0[5]: Copied! <pre>dept_info = next(x for x in table_info if x[\"Name\"] == \"department\")\ndept_location = dept_info[\"StorageDescriptor\"][\"Location\"]\nwr.s3.list_objects(dept_location)\n</pre> dept_info = next(x for x in table_info if x[\"Name\"] == \"department\") dept_location = dept_info[\"StorageDescriptor\"][\"Location\"] wr.s3.list_objects(dept_location) Out[5]: <pre>['s3://alpha-everyone/mratford/source_db/department/be5d0e3a417a4c7bbb0bf0ff5996bba4.snappy.parquet']</pre> <p>Now delete the <code>department</code> table.</p> In\u00a0[6]: Copied! <pre>pydb.delete_table_and_data(database=db_name, table=\"department\")\n</pre> pydb.delete_table_and_data(database=db_name, table=\"department\") Out[6]: <pre>True</pre> <p>Check that it's no longer in the database.</p> In\u00a0[7]: Copied! <pre>table_info = list(wr.catalog.get_tables(database=db_name))\n[x[\"Name\"] for x in table_info]\n</pre> table_info = list(wr.catalog.get_tables(database=db_name)) [x[\"Name\"] for x in table_info] Out[7]: <pre>['employees', 'sales']</pre> <p>Check that the data no longer exists.</p> In\u00a0[8]: Copied! <pre>wr.s3.list_objects(dept_location)\n</pre> wr.s3.list_objects(dept_location) Out[8]: <pre>[]</pre> In\u00a0[9]: Copied! <pre>wr.catalog.get_partitions(database=db_name, table=\"sales\")\n</pre> wr.catalog.get_partitions(database=db_name, table=\"sales\") Out[9]: <pre>{'s3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=2/': ['3',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=3/': ['1',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=4/': ['2',\n  '4'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=4/': ['1',\n  '4'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=3/': ['2',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=4/': ['4',\n  '4'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=1/': ['2',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=2/': ['2',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=3/': ['4',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=1/': ['4',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=2/': ['4',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=2/': ['1',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=1/': ['1',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=1/': ['3',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=4/': ['3',\n  '4'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=3/': ['3',\n  '3']}</pre> <p>Check the data for one of the partitions.</p> In\u00a0[10]: Copied! <pre>wr.s3.list_objects(f\"{source_db_base_path}sales/employee_id=1/qtr=4/\")\n</pre> wr.s3.list_objects(f\"{source_db_base_path}sales/employee_id=1/qtr=4/\") Out[10]: <pre>['s3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=4/a59b17a35bbf47d1bb2def454fb3bbc6.snappy.parquet']</pre> <p>Use an SQL like query to delete the partition and data for quarter 4.</p> In\u00a0[11]: Copied! <pre>pydb.delete_partitions_and_data(\n    database=db_name, table=\"sales\", expression=\"qtr = 4\"\n)\n</pre> pydb.delete_partitions_and_data(     database=db_name, table=\"sales\", expression=\"qtr = 4\" ) <p>Check that the partition no longer exists.</p> In\u00a0[12]: Copied! <pre>wr.catalog.get_partitions(database=db_name, table=\"sales\")\n</pre> wr.catalog.get_partitions(database=db_name, table=\"sales\") Out[12]: <pre>{'s3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=2/': ['3',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=3/': ['1',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=3/': ['2',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=1/': ['2',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=2/': ['2',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=3/': ['4',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=1/': ['4',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=2/': ['4',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=2/': ['1',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=1/': ['1',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=1/': ['3',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=3/': ['3',\n  '3']}</pre> <p>Check that the data no longer exists.</p> In\u00a0[13]: Copied! <pre>wr.s3.list_objects(f\"{source_db_base_path}sales/employee_id=1/qtr=4/\")\n</pre> wr.s3.list_objects(f\"{source_db_base_path}sales/employee_id=1/qtr=4/\") Out[13]: <pre>[]</pre> <p>Using a more complex query, delete quarters 1 and 2 for employee 3.</p> In\u00a0[14]: Copied! <pre>pydb.delete_partitions_and_data(\n    database=db_name, table=\"sales\", expression=\"employee_id = 3 and qtr &lt; 3\"\n)\nwr.catalog.get_partitions(database=db_name, table=\"sales\")\n</pre> pydb.delete_partitions_and_data(     database=db_name, table=\"sales\", expression=\"employee_id = 3 and qtr &lt; 3\" ) wr.catalog.get_partitions(database=db_name, table=\"sales\") Out[14]: <pre>{'s3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=3/': ['1',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=3/': ['2',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=1/': ['2',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=2/qtr=2/': ['2',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=3/': ['4',\n  '3'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=1/': ['4',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=4/qtr=2/': ['4',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=2/': ['1',\n  '2'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=1/qtr=1/': ['1',\n  '1'],\n 's3://alpha-everyone/mratford/source_db/sales/employee_id=3/qtr=3/': ['3',\n  '3']}</pre> <p>See the documentation for details on the expression syntax.</p> In\u00a0[15]: Copied! <pre>db_name in list(wr.catalog.databases()[\"Database\"])\n</pre> db_name in list(wr.catalog.databases()[\"Database\"]) Out[15]: <pre>True</pre> In\u00a0[16]: Copied! <pre>pydb.delete_database_and_data(db_name)\n</pre> pydb.delete_database_and_data(db_name) Out[16]: <pre>True</pre> In\u00a0[17]: Copied! <pre>db_name in list(wr.catalog.databases()[\"Database\"])\n</pre> db_name in list(wr.catalog.databases()[\"Database\"]) Out[17]: <pre>False</pre> In\u00a0[18]: Copied! <pre>wr.s3.list_objects(source_db_base_path)\n</pre> wr.s3.list_objects(source_db_base_path) Out[18]: <pre>[]</pre> In\u00a0[19]: Copied! <pre>source_db_base_path\n</pre> source_db_base_path Out[19]: <pre>'s3://alpha-everyone/mratford/source_db/'</pre> In\u00a0[20]: Copied! <pre># Setup source database\n# Create the database\nwr.catalog.create_database(db_name, exist_ok=True)\n\n# Iterate through the tables in data/ and write them to our db using awswrangler\nfor table_name in [\"department\", \"employees\", \"sales\"]:\n\n    df = pd.read_csv(f\"data/{table_name}.csv\")\n    table_path = os.path.join(source_db_base_path, f\"{table_name}/\")\n    wr.s3.to_parquet(\n        df=df,\n        path=table_path,\n        index=False,\n        dataset=True,  # True allows the other params below i.e. overwriting to db.table\n        database=db_name,\n        table=table_name,\n        mode=\"overwrite\",\n    )\n\nsql = f\"\"\"\nSELECT employee_id, sum(sales) as total_sales\nFROM {db_name}.sales\nGROUP BY employee_id\n\"\"\"\nprint(sql)\npydb.create_temp_table(sql, table_name=\"total_sales\")\n</pre> # Setup source database # Create the database wr.catalog.create_database(db_name, exist_ok=True)  # Iterate through the tables in data/ and write them to our db using awswrangler for table_name in [\"department\", \"employees\", \"sales\"]:      df = pd.read_csv(f\"data/{table_name}.csv\")     table_path = os.path.join(source_db_base_path, f\"{table_name}/\")     wr.s3.to_parquet(         df=df,         path=table_path,         index=False,         dataset=True,  # True allows the other params below i.e. overwriting to db.table         database=db_name,         table=table_name,         mode=\"overwrite\",     )  sql = f\"\"\" SELECT employee_id, sum(sales) as total_sales FROM {db_name}.sales GROUP BY employee_id \"\"\" print(sql) pydb.create_temp_table(sql, table_name=\"total_sales\") <pre>\nSELECT employee_id, sum(sales) as total_sales\nFROM aws_example_mratford.sales\nGROUP BY employee_id\n\n</pre> In\u00a0[21]: Copied! <pre>pydb.read_sql_query(\"select * from __temp__.total_sales\")\n</pre> pydb.read_sql_query(\"select * from __temp__.total_sales\") Out[21]: employee_id total_sales 0 21 1643.14 1 93 688.05 2 101 817.45 3 60 1331.55 4 17 2302.02 5 5 2480.50 6 28 2071.77 7 6 2207.77 8 26 1994.65 9 3 2590.60 10 44 2184.14 11 12 2286.28 12 42 1688.76 13 20 2851.36 14 10 1935.67 15 32 2693.30 16 7 2465.13 17 19 2442.86 18 24 2248.35 19 220 1377.37 20 1 2911.65 21 23 3036.47 22 38 2158.55 23 11 3092.89 24 59 927.30 25 43 2073.25 26 45 2778.84 27 37 1984.24 28 39 1931.27 29 46 2547.63 30 4 2996.54 31 63 721.91 32 9 2279.84 33 35 2624.64 34 16 2373.06 35 47 1489.52 36 200 1108.89 37 57 953.09 38 13 2711.01 39 25 3099.01 40 31 2461.53 41 33 2685.07 42 80 611.18 43 34 1387.50 44 30 2251.47 45 29 2478.25 46 27 3050.02 47 2 1785.73 48 8 2155.77 49 15 2613.67 50 41 1326.88 51 40 2595.53 52 36 1580.06 53 18 1759.39 In\u00a0[37]: Copied! <pre>pydb.delete_database_and_data(\"__temp__\")\n</pre> pydb.delete_database_and_data(\"__temp__\") Out[37]: <pre>True</pre> In\u00a0[38]: Copied! <pre>try:\n    df = pydb.read_sql_query(\"select * from __temp__.total_sales\")\n    print(\"Error, temporary database not deleted correctly.\")\nexcept wr.exceptions.QueryFailed:\n    print(\"Query failed correctly.\")\n</pre> try:     df = pydb.read_sql_query(\"select * from __temp__.total_sales\")     print(\"Error, temporary database not deleted correctly.\") except wr.exceptions.QueryFailed:     print(\"Query failed correctly.\") <pre>Query failed correctly.\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/delete_databases_tables_and_partitions/#delete-databases-tables-and-partitions-and-data","title":"Delete databases, tables and partitions and data\u00b6","text":"<p><code>pydbtools</code> now has functions to remove databases, database tables, and partitions, plus the underlying data on S3.</p>"},{"location":"examples/delete_databases_tables_and_partitions/#setup","title":"Setup\u00b6","text":"<p>First run the following cells to set up the database tables.</p> <p>Import the necessary libraries.</p>"},{"location":"examples/delete_databases_tables_and_partitions/#deleting-a-table","title":"Deleting a table\u00b6","text":"<p>Show the tables in the database.</p>"},{"location":"examples/delete_databases_tables_and_partitions/#deleting-a-partition","title":"Deleting a partition\u00b6","text":"<p>Show the partitions from the <code>sales</code> table.</p>"},{"location":"examples/delete_databases_tables_and_partitions/#deleting-a-database","title":"Deleting a database\u00b6","text":""},{"location":"examples/delete_databases_tables_and_partitions/#deleting-temporary-database-tables","title":"Deleting temporary database tables\u00b6","text":"<p>It might be useful during development to get rid of the temporary database or one of it's tables if something has gone wrong. This can be accomplished by using <code>__temp__</code> as the database name in one of the functions above.</p>"},{"location":"examples/displaying_tables_example/","title":"Displaying databases, tables and columns using tables function","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport awswrangler as wr\nimport datetime\nimport pydbtools as pydb\n</pre> import pandas as pd import awswrangler as wr import datetime import pydbtools as pydb <p>The <code>tables</code> function calls the aws wrangler function <code>wrangler.catalog.tables</code> which has the following paramaters.</p> <p>Parameters:</p> <ul> <li>limit (int, optional) \u2013 Max number of tables to be returned. If none is provided, the default value is 100.</li> <li>catalog_id (str, optional) \u2013 The ID of the Data Catalog from which to retrieve Databases. If none is provided, the AWS account ID is used by default.</li> <li>database (str, optional) \u2013 Database name.</li> <li>transaction_id (str, optional) \u2013 The ID of the transaction (i.e. used with GOVERNED tables).</li> <li>search_text (str, optional) \u2013 Select only tables with the given string in table\u2019s properties.</li> <li>name_contains (str, optional) \u2013 Select by a specific string on table name</li> <li>name_prefix (str, optional) \u2013 Select by a specific prefix on table name</li> <li>name_suffix (str, optional) \u2013 Select by a specific suffix on table name</li> <li>boto3_session (boto3.Session(), optional) \u2013 Boto3 Session. The default boto3 session will be used if boto3_session receive None.</li> </ul> <p>Returns: Pandas Dataframe filled by formatted infos.</p> <p>https://aws-sdk-pandas.readthedocs.io/en/stable/stubs/awswrangler.catalog.tables.html</p> <p>If called on it's own, <code>tables()</code> will return the top 100 tables as a dataframe without any filters.</p> In\u00a0[2]: Copied! <pre>pydb.tables()\n</pre> pydb.tables() Out[2]: Database Table Description TableType Columns Partitions 0 accom_and_employ_qa_monitoring sl014_table EXTERNAL_TABLE period, prison_nomis, prison_nd, nps_or_crc, n... 1 accom_and_employ_qa_monitoring sl016_table EXTERNAL_TABLE period, prison_nomis, prison_nd, nps_or_crc, n... 2 accom_and_employ_qa_monitoring test_remmc1020 EXTERNAL_TABLE month, year 3 address_base_test address_base_json EXTERNAL_TABLE changetype, startdate, lastupdatedate, entryda... snapshot, tile 4 address_base_test address_base_parquet EXTERNAL_TABLE changetype, startdate, lastupdatedate, entryda... snapshot, tile ... ... ... ... ... ... ... 95 alpha_ccs_temp e2e_time_xhibit_backup20220721 EXTERNAL_TABLE defendant_on_case_id, case_id, year, quarter, ... 96 alpha_ccs_temp e2e_time_xhibit_backup20220818 EXTERNAL_TABLE defendant_on_case_id, case_id, year, quarter, ... 97 alpha_ccs_temp e2e_time_xhibit_backup20220927 EXTERNAL_TABLE defendant_on_case_id, case_id, year, quarter, ... 98 alpha_ccs_temp e2e_time_xhibit_backup20221022 EXTERNAL_TABLE defendant_on_case_id, case_id, year, quarter, ... 99 alpha_ccs_temp e2e_time_xhibit_backup20221022_2 EXTERNAL_TABLE defendant_on_case_id, case_id, year, quarter, ... <p>100 rows \u00d7 6 columns</p> <p>Most of the time, we are interested in the tables of a specific database. This can be specified using the database argument. If the database has a large number of tables, the limit will also need to be adjusted. If <code>limit=None</code>, there is no limit. The other arguments which can be used are described above.</p> In\u00a0[3]: Copied! <pre>pydb.tables(database=\"alpha_ccs_temp\", limit=None)\n</pre> pydb.tables(database=\"alpha_ccs_temp\", limit=None) Out[3]: Database Table Description TableType Columns Partitions 0 alpha_ccs_temp ccs_disposals_receipts EXTERNAL_TABLE disposal_year, disposal_quarter, disposal_mont... 1 alpha_ccs_temp ccs_disposals_receipts_post_sept21 EXTERNAL_TABLE disposal_year, disposal_quarter, disposal_mont... 2 alpha_ccs_temp ccs_disposals_receipts_pre_sept21 EXTERNAL_TABLE disposal_year, disposal_quarter, disposal_mont... 3 alpha_ccs_temp ccs_disposals_receipts_sdp_v2 EXTERNAL_TABLE disposal_year, disposal_quarter, disposal_mont... 4 alpha_ccs_temp ccs_disposals_receipts_sdp_v3 EXTERNAL_TABLE disposal_year, disposal_quarter, disposal_mont... ... ... ... ... ... ... ... 111 alpha_ccs_temp opt_sitting_days_septemberv4 EXTERNAL_TABLE crn_id, crn_name, lcjb_area_id, lcjb_area_name... 112 alpha_ccs_temp os_definitions EXTERNAL_TABLE variable, definition 113 alpha_ccs_temp test EXTERNAL_TABLE defendant_on_case_id, case_id, year, quarter, ... 114 alpha_ccs_temp test_sdp_v1_mags_sjp_time_cp_20221020 EXTERNAL_TABLE sdp_v1_flag, case_id, defendant_id, offence_de... 115 alpha_ccs_temp timeliness EXTERNAL_TABLE year, quarter, age, n, valid_n, valid_n_and_cc... <p>116 rows \u00d7 6 columns</p>"},{"location":"examples/displaying_tables_example/#displaying-databases-tables-and-columns-using-tables-function","title":"Displaying databases, tables and columns using tables function\u00b6","text":"<p>In this tutorial we are going to use <code>tables</code> to explore databases, tables, and columns stored in S3.</p>"},{"location":"examples/mojap_tools_demo/","title":"MoJ AP tools demo","text":"In\u00a0[15]: Copied! <pre>import pydbtools as pydb\nfrom arrow_pd_parser import reader, writer\nfrom mojap_metadata import Metadata\nimport pandas as pd\nimport awswrangler as wr\nimport itertools\n\npd.options.display.max_columns = None\n</pre> import pydbtools as pydb from arrow_pd_parser import reader, writer from mojap_metadata import Metadata import pandas as pd import awswrangler as wr import itertools  pd.options.display.max_columns = None <p>Create a new database, cleaning up any tables and data beforehand in case it already exists.</p> In\u00a0[16]: Copied! <pre>db = \"dmet_example\"\n\npydb.delete_database_and_data(db)\npydb.create_database(db)\n</pre> db = \"dmet_example\"  pydb.delete_database_and_data(db) pydb.create_database(db) Out[16]: <pre>True</pre> <p>We have a dataset that consists of a number of very large csv files. How can we load this without running out of memory and crashing our session?</p> In\u00a0[17]: Copied! <pre>big_path = \"s3://alpha-everyone/s3_data_packer_test/land/big/\"\n\n# Don't run this!\n# df = wr.s3.read_csv(big_path)\n</pre> big_path = \"s3://alpha-everyone/s3_data_packer_test/land/big/\"  # Don't run this! # df = wr.s3.read_csv(big_path) <p><code>arrow_pd_parser</code> has the ability to read files in chunks, returning an iterator of dataframes. Specify a number of lines to load with chunksize to preview the table.</p> In\u00a0[18]: Copied! <pre>df = next(reader.read(big_path, file_format=\"csv\", chunksize=10, index_col=0))\ndf\n</pre> df = next(reader.read(big_path, file_format=\"csv\", chunksize=10, index_col=0)) df Out[18]: name email address city state date_time price 0 Kip Love gar1812@yahoo.com 315 Fairfield Pike Germantown Maine 2016-05-07 03:50:50.991892 426 1 Joel Clements intendancies1853@outlook.com 211 Garces Gate Canton South Carolina 2007-03-20 17:29:35.856517 1780 2 Tambra Bowman badju1819@gmail.com 604 Paulding Brae Roanoke Rapids Missouri 2011-05-11 13:23:57.802563 1919 3 Claude Jackson clannishly1990@live.com 1084 Lyndhurst Crescent Porterville Arkansas 2019-05-25 20:22:14.927012 1404 4 Percy Sullivan simoniac1930@live.com 34 Beaver Row Lake Jackson Hawaii 2013-09-21 00:26:37.106366 126 5 Nathan Frank unstocking1800@yahoo.com 151 Music Concourse Gardens Jackson Maryland 2004-11-18 20:36:42.006247 417 6 Leonel Buck amazing1959@outlook.com 258 Higuera Line Gurnee Illinois 2016-03-27 12:33:38.784997 938 7 Lawanna Hess rovers1805@gmail.com 1065 Freeman Townline Inglewood Kansas 2006-04-11 19:15:28.331296 782 8 Carmelo Morgan piperazine1959@protonmail.com 412 San Buenaventura Turnpike Rolling Meadows Ohio 2004-05-22 14:03:06.479138 185 9 Shiloh Silva bibliomane1826@yahoo.com 917 Brookdale Path Bainbridge Island Oklahoma 2000-12-08 08:47:10.598186 478 <p>Checking the data types we can see that <code>date_time</code> is a string but we would like it to be a timestamp.</p> In\u00a0[19]: Copied! <pre>df.dtypes\n</pre> df.dtypes Out[19]: <pre>name         string\nemail        string\naddress      string\ncity         string\nstate        string\ndate_time    string\nprice         Int64\ndtype: object</pre> <p>Create metadata to fix this using <code>mojap_metadata.Metadata</code>. Note that <code>arrow</code> rather than <code>pandas</code> types are used, and these will be enforced across formats.</p> In\u00a0[20]: Copied! <pre>metadata = Metadata.from_dict(\n    {\n        \"name\": \"big_table\",\n        \"columns\": [\n            {\"name\": n, \"type\": t}\n            for n, t in [\n                (\"name\", \"string\"),\n                (\"email\", \"string\"),\n                (\"address\", \"string\"),\n                (\"city\", \"string\"),\n                (\"state\", \"string\"),\n                (\"date_time\", \"timestamp(ms)\"),\n                (\"price\", \"int64\"),\n            ]\n        ],\n    }\n)\nmetadata.columns\n</pre> metadata = Metadata.from_dict(     {         \"name\": \"big_table\",         \"columns\": [             {\"name\": n, \"type\": t}             for n, t in [                 (\"name\", \"string\"),                 (\"email\", \"string\"),                 (\"address\", \"string\"),                 (\"city\", \"string\"),                 (\"state\", \"string\"),                 (\"date_time\", \"timestamp(ms)\"),                 (\"price\", \"int64\"),             ]         ],     } ) metadata.columns Out[20]: <pre>[{'name': 'name', 'type': 'string'},\n {'name': 'email', 'type': 'string'},\n {'name': 'address', 'type': 'string'},\n {'name': 'city', 'type': 'string'},\n {'name': 'state', 'type': 'string'},\n {'name': 'date_time', 'type': 'timestamp(ms)'},\n {'name': 'price', 'type': 'int64'}]</pre> <p>Now try previewing the data again with metadata enforced.</p> In\u00a0[21]: Copied! <pre>df = next(\n    reader.read(\n        big_path,\n        file_format=\"csv\",\n        chunksize=10,\n        index_col=0,\n        metadata=metadata,\n    )\n)\ndf\n</pre> df = next(     reader.read(         big_path,         file_format=\"csv\",         chunksize=10,         index_col=0,         metadata=metadata,     ) ) df Out[21]: name email address city state date_time price 0 Kip Love gar1812@yahoo.com 315 Fairfield Pike Germantown Maine 2016-05-07 03:50:50.991892 426 1 Joel Clements intendancies1853@outlook.com 211 Garces Gate Canton South Carolina 2007-03-20 17:29:35.856517 1780 2 Tambra Bowman badju1819@gmail.com 604 Paulding Brae Roanoke Rapids Missouri 2011-05-11 13:23:57.802563 1919 3 Claude Jackson clannishly1990@live.com 1084 Lyndhurst Crescent Porterville Arkansas 2019-05-25 20:22:14.927012 1404 4 Percy Sullivan simoniac1930@live.com 34 Beaver Row Lake Jackson Hawaii 2013-09-21 00:26:37.106366 126 5 Nathan Frank unstocking1800@yahoo.com 151 Music Concourse Gardens Jackson Maryland 2004-11-18 20:36:42.006247 417 6 Leonel Buck amazing1959@outlook.com 258 Higuera Line Gurnee Illinois 2016-03-27 12:33:38.784997 938 7 Lawanna Hess rovers1805@gmail.com 1065 Freeman Townline Inglewood Kansas 2006-04-11 19:15:28.331296 782 8 Carmelo Morgan piperazine1959@protonmail.com 412 San Buenaventura Turnpike Rolling Meadows Ohio 2004-05-22 14:03:06.479138 185 9 Shiloh Silva bibliomane1826@yahoo.com 917 Brookdale Path Bainbridge Island Oklahoma 2000-12-08 08:47:10.598186 478 <p>Note that <code>date_time</code> is now an object of <code>datetime.datetime</code> type as the <code>pandas</code> date/time types have too narrow a range.</p> In\u00a0[22]: Copied! <pre>df.dtypes\n</pre> df.dtypes Out[22]: <pre>name         string\nemail        string\naddress      string\ncity         string\nstate        string\ndate_time    object\nprice         Int64\ndtype: object</pre> <p>For the sake of this demo take a small slice, the first 5 chunks, of an iterator reading the whole data set.</p> In\u00a0[23]: Copied! <pre>r = itertools.islice(\n    reader.read(\n        big_path,\n        file_format=\"csv\",\n        chunksize=\"100MB\",\n        index_col=0,\n        metadata=metadata,\n    ),\n    5,\n)\n</pre> r = itertools.islice(     reader.read(         big_path,         file_format=\"csv\",         chunksize=\"100MB\",         index_col=0,         metadata=metadata,     ),     5, ) <p>We can then convert between formats, in this case to parquet, while preserving the metadata.</p> In\u00a0[24]: Copied! <pre>new_path = \"s3://alpha-everyone/dmet_st/big_table.parquet\"\nwr.s3.delete_objects(new_path)\nwriter.write(r, new_path, metadata=metadata)\n</pre> new_path = \"s3://alpha-everyone/dmet_st/big_table.parquet\" wr.s3.delete_objects(new_path) writer.write(r, new_path, metadata=metadata) <p>Big datasets in S3 can be used to create a queryable table.</p> In\u00a0[25]: Copied! <pre>pydb.file_to_table(\n    new_path,\n    database=db,\n    table=\"big_table\",\n    location=\"s3://alpha-everyone/dmet_st/dmet_example/big_table\",\n    chunksize=\"100MB\",\n    metadata=metadata,\n)\npydb.read_sql_query(f\"select * from {db}.big_table limit 5\")\n</pre> pydb.file_to_table(     new_path,     database=db,     table=\"big_table\",     location=\"s3://alpha-everyone/dmet_st/dmet_example/big_table\",     chunksize=\"100MB\",     metadata=metadata, ) pydb.read_sql_query(f\"select * from {db}.big_table limit 5\") Out[25]: name email address city state date_time price 0 Werner Jenkins congruism1828@gmail.com 497 Sydney Path Lodi Delaware 2012-03-18 08:15:24.496 1485 1 Jacques Garner embrothelled1973@yahoo.com 1244 West Point High Street Elmira Michigan 2002-09-05 00:39:12.638 463 2 Freeman Callahan wonders1955@protonmail.com 869 Neptune Turnpike Burbank Michigan 2006-05-24 06:33:02.054 161 3 Reuben Ewing charmian2048@yahoo.com 823 Minnesota Freeway Anacortes Minnesota 2011-04-17 19:49:33.711 777 4 Dwain Maynard vulturelike1889@yahoo.com 935 North Hughes Terrace Mamaroneck Oregon 2011-06-25 06:16:58.032 1105 <p>Create a new table in the database from an SQL statement.</p> In\u00a0[26]: Copied! <pre>pydb.delete_table_and_data(database=db, table=\"state_revenues\")\n\npydb.create_table(\n    f\"\"\"\n    select state, sum(price) as revenue\n    from {db}.big_table\n    group by state\n    \"\"\",\n    database=db,\n    table=\"state_revenues\",\n    location=\"s3://alpha-everyone/dmet_st/dmet_example/state_revenues\",\n)\n\nsr = pydb.read_sql_query(f\"select * from {db}.state_revenues\")\nsr\n</pre> pydb.delete_table_and_data(database=db, table=\"state_revenues\")  pydb.create_table(     f\"\"\"     select state, sum(price) as revenue     from {db}.big_table     group by state     \"\"\",     database=db,     table=\"state_revenues\",     location=\"s3://alpha-everyone/dmet_st/dmet_example/state_revenues\", )  sr = pydb.read_sql_query(f\"select * from {db}.state_revenues\") sr Out[26]: state revenue 0 North Dakota 19936459 1 Pennsylvania 19876171 2 Montana 20228848 3 Maine 19783670 4 Washington 20056159 5 Tennessee 20081268 6 Ohio 20222361 7 South Carolina 20161898 8 Texas 20095812 9 Mississippi 19881812 10 Connecticut 20104647 11 Oregon 20184460 12 Florida 20011580 13 Iowa 20227238 14 Nevada 20112272 15 Alaska 19822735 16 Illinois 20175380 17 Colorado 20197961 18 West Virginia 19938810 19 Oklahoma 19899153 20 Missouri 20086612 21 Rhode Island 20075477 22 Wyoming 20129612 23 New Jersey 19833567 24 New York 20216151 25 Delaware 19825190 26 Vermont 20570793 27 Idaho 20328920 28 Virginia 20243585 29 Nebraska 19912395 30 Maryland 20206547 31 Kentucky 20248912 32 New Mexico 19974839 33 Louisiana 20076739 34 Arizona 20051478 35 North Carolina 20144047 36 South Dakota 20024755 37 Arkansas 20429413 38 Michigan 20237562 39 Hawaii 19992112 40 California 20028102 41 Georgia 20152913 42 Massachusetts 20141289 43 Utah 20167422 44 Kansas 20375149 45 Wisconsin 19983479 46 Alabama 20138405 47 Minnesota 19840186 48 Indiana 20425105 49 New Hampshire 20156172 <p>What if we want to do some manipulation with pandas?</p> In\u00a0[27]: Copied! <pre>starts_with_n = sr[sr[\"state\"].str.startswith(\"N\")]\nstarts_with_n\n</pre> starts_with_n = sr[sr[\"state\"].str.startswith(\"N\")] starts_with_n Out[27]: state revenue 0 North Dakota 19936459 14 Nevada 20112272 23 New Jersey 19833567 24 New York 20216151 29 Nebraska 19912395 32 New Mexico 19974839 35 North Carolina 20144047 49 New Hampshire 20156172 <p>We can then create another table in the database from the manipulated dataframe. This allows us to create a hybrid pipeline of SQL and pandas operations.</p> In\u00a0[28]: Copied! <pre>pydb.dataframe_to_table(\n    starts_with_n,\n    database=db,\n    table=\"starts_with_n\",\n    location=\"s3://alpha-everyone/dmet_st/dmet_example/starts_with_n\",\n)\n\npydb.read_sql_query(f\"select * from {db}.starts_with_n\")\n</pre> pydb.dataframe_to_table(     starts_with_n,     database=db,     table=\"starts_with_n\",     location=\"s3://alpha-everyone/dmet_st/dmet_example/starts_with_n\", )  pydb.read_sql_query(f\"select * from {db}.starts_with_n\") Out[28]: state revenue 0 North Dakota 19936459 1 Nevada 20112272 2 New Jersey 19833567 3 New York 20216151 4 Nebraska 19912395 5 New Mexico 19974839 6 North Carolina 20144047 7 New Hampshire 20156172 In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/mojap_tools_demo/#moj-ap-tools-demo","title":"MoJ AP tools demo\u00b6","text":"<p>This notebook demonstrates the use of some of the Python tools developed by data engineers to make creating analytical pipelines simpler for data scientists and analysts.</p> <p>It focuses on taking a large dataset which is too big for memory, converting it to another format while applying metadata to ensure consistent data types, and creating a database with tables from files or dataframes.</p> <p>First import the necessary libraries. pydbtools, arrow_pd_parser and mojap_metadata are libraries created and maintained by the Data Modelling and Engineering Team.</p>"},{"location":"examples/simple_database_creation_manipulation/","title":"Simple Database Creation and Manipulation","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport awswrangler as wr\nimport datetime\nimport pydbtools as pydb\n</pre> import pandas as pd import awswrangler as wr import datetime import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ndb_name = f\"aws_example_{foldername}\"\ndb_base_path = f\"s3://{bucketname}/{foldername}/database\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    wr.catalog.delete_database(name=db_name)\n</pre> bucketname = \"alpha-everyone\" db_name = f\"aws_example_{foldername}\" db_base_path = f\"s3://{bucketname}/{foldername}/database\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     wr.catalog.delete_database(name=db_name) <pre>deleting objs\n</pre> In\u00a0[4]: Copied! <pre>df = pd.read_csv(\"data/employees.csv\")\ndf.head()\n</pre> df = pd.read_csv(\"data/employees.csv\") df.head() Out[4]: employee_id sex forename surname department_id manager_id 0 1 M Dexter Mitchell 1.0 17.0 1 2 F Summer Bennett 1.0 17.0 2 3 M Pip Carter 1.0 17.0 3 4 F Bella Long 1.0 17.0 4 5 F Lexie Perry NaN 17.0 In\u00a0[5]: Copied! <pre>df[\"creation_date\"] = datetime.date(2021, 1, 1)\ndf.head()\n</pre> df[\"creation_date\"] = datetime.date(2021, 1, 1) df.head() Out[5]: employee_id sex forename surname department_id manager_id creation_date 0 1 M Dexter Mitchell 1.0 17.0 2021-01-01 1 2 F Summer Bennett 1.0 17.0 2021-01-01 2 3 M Pip Carter 1.0 17.0 2021-01-01 3 4 F Bella Long 1.0 17.0 2021-01-01 4 5 F Lexie Perry NaN 17.0 2021-01-01 In\u00a0[6]: Copied! <pre># Create the database\nwr.catalog.create_database(db_name)\n\n# note table_path is a folder as glue treats all the\n# data in a folder as contents of a single table\ntable_path = f\"{db_base_path}/employees/\"\n\n# Write your pandas dataframe to S3 and add it as a table in your database\nwr.s3.to_parquet(\n    df=df,\n    path=table_path,\n    index=False,\n    dataset=True,  # True allows the other params below i.e. overwriting to db.table\n    database=db_name,\n    table=\"employees\",\n    mode=\"overwrite\",\n)\n</pre> # Create the database wr.catalog.create_database(db_name)  # note table_path is a folder as glue treats all the # data in a folder as contents of a single table table_path = f\"{db_base_path}/employees/\"  # Write your pandas dataframe to S3 and add it as a table in your database wr.s3.to_parquet(     df=df,     path=table_path,     index=False,     dataset=True,  # True allows the other params below i.e. overwriting to db.table     database=db_name,     table=\"employees\",     mode=\"overwrite\", ) Out[6]: <pre>{'paths': ['s3://alpha-everyone/mratford/database/employees/bc076712bce54b45b3a6ec7f8def198f.snappy.parquet'],\n 'partitions_values': {}}</pre> In\u00a0[7]: Copied! <pre>df[\"creation_date\"] = datetime.date(2021, 1, 1)\n\ndf[\"new_col1\"] = df[\"employee_id\"] + 100\ndf[\"new_col2\"] = \"some text\"\n\ndf.head()\n</pre> df[\"creation_date\"] = datetime.date(2021, 1, 1)  df[\"new_col1\"] = df[\"employee_id\"] + 100 df[\"new_col2\"] = \"some text\"  df.head() Out[7]: employee_id sex forename surname department_id manager_id creation_date new_col1 new_col2 0 1 M Dexter Mitchell 1.0 17.0 2021-01-01 101 some text 1 2 F Summer Bennett 1.0 17.0 2021-01-01 102 some text 2 3 M Pip Carter 1.0 17.0 2021-01-01 103 some text 3 4 F Bella Long 1.0 17.0 2021-01-01 104 some text 4 5 F Lexie Perry NaN 17.0 2021-01-01 105 some text In\u00a0[8]: Copied! <pre># Write the new data to S3.\n# Note the only thing has changed is mode=\"append\" whereas previously it was mode=\"overwrite\"\nwr.s3.to_parquet(\n    df=df,\n    path=table_path,\n    index=False,\n    dataset=True,\n    database=db_name,\n    table=\"employees\",\n    mode=\"append\",\n)\n</pre> # Write the new data to S3. # Note the only thing has changed is mode=\"append\" whereas previously it was mode=\"overwrite\" wr.s3.to_parquet(     df=df,     path=table_path,     index=False,     dataset=True,     database=db_name,     table=\"employees\",     mode=\"append\", ) Out[8]: <pre>{'paths': ['s3://alpha-everyone/mratford/database/employees/11667f33cb814d039b588eee1a206d75.snappy.parquet'],\n 'partitions_values': {}}</pre> In\u00a0[9]: Copied! <pre># Each uploaded dataset had one employee with an employee_id == 1\n# So lets pull that down to demonstrate both tables were added to the data\nsql = f\"SELECT * from {db_name}.employees where employee_id = 1\"\ndb_table = pydb.read_sql_query(sql, ctas_approach=False)\n</pre> # Each uploaded dataset had one employee with an employee_id == 1 # So lets pull that down to demonstrate both tables were added to the data sql = f\"SELECT * from {db_name}.employees where employee_id = 1\" db_table = pydb.read_sql_query(sql, ctas_approach=False) In\u00a0[10]: Copied! <pre>print(sql)\n</pre> print(sql) <pre>SELECT * from aws_example_mratford.employees where employee_id = 1\n</pre> In\u00a0[11]: Copied! <pre>db_table.head()\n</pre> db_table.head() Out[11]: employee_id sex forename surname department_id manager_id creation_date new_col1 new_col2 0 1 M Dexter Mitchell 1.0 17.0 2021-01-01 101 some text 1 1 M Dexter Mitchell 1.0 17.0 2021-01-01 &lt;NA&gt; &lt;NA&gt; In\u00a0[12]: Copied! <pre>### Clean up\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"deleting {db_name}\")\n    wr.catalog.delete_database(name=db_name)\n</pre> ### Clean up  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     print(f\"deleting {db_name}\")     wr.catalog.delete_database(name=db_name) <pre>deleting objs\ndeleting aws_example_mratford\n</pre> In\u00a0[13]: Copied! <pre># Demonstrate db no longer exists\ndb_name in wr.catalog.databases()[\"Database\"].to_list()\n</pre> # Demonstrate db no longer exists db_name in wr.catalog.databases()[\"Database\"].to_list() Out[13]: <pre>False</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/simple_database_creation_manipulation/#simple-database-creation-and-manipulation","title":"Simple Database Creation and Manipulation\u00b6","text":"<p>In this tutorial we are going to use aws-wrangler to create a database of different tables.</p> <p>Let's create a database out of the test data <code>employees.csv</code>, <code>sales.csv</code> and <code>department.csv</code> (all in the <code>data/</code> folder)</p> <p>Note this is basically taken from: https://github.com/awslabs/aws-data-wrangler/blob/master/tutorials/014%20-%20Schema%20Evolution.ipynb</p>"},{"location":"examples/simple_database_creation_manipulation/#setup-first","title":"Setup first\u00b6","text":""},{"location":"examples/simple_database_creation_manipulation/#lets-get-the-data-in-pandas-first","title":"Lets get the data in pandas first\u00b6","text":""},{"location":"examples/simple_database_creation_manipulation/#lets-do-some-transforms-on-it","title":"Lets do some transforms on it\u00b6","text":""},{"location":"examples/simple_database_creation_manipulation/#write-the-table-to-a-database","title":"write the table to a database\u00b6","text":"<p>parquet is always your best bet for writing data to a Glue Database especially if you only want to retrieve that data via Athena SQL queries.</p>"},{"location":"examples/simple_database_creation_manipulation/#append-new-data-to-the-table","title":"Append new data to the table\u00b6","text":"<p>Let's for fun also add new cols as well</p>"},{"location":"examples/simple_database_creation_manipulation/#now-query-the-data-with-athena-to-look-at-it","title":"Now query the data with Athena to look at it\u00b6","text":"<p>This should use pydbtools rather than aws_wrangler (if you are a AP user).</p>"},{"location":"examples/sql_templating/","title":"Using SQL templating with pydbtools","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\ndb_name = f\"aws_example_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"{db_name} found deleting\")\n    wr.catalog.delete_database(name=db_name)\n\n# Setup source database\n# Create the database\nwr.catalog.create_database(db_name)\n\n# Iterate through the tables in data/ and write them to our db using awswrangler\nfor table_name in [\"department\", \"employees\", \"sales\"]:\n\n    df = pd.read_csv(f\"data/{table_name}.csv\")\n    table_path = os.path.join(source_db_base_path, f\"{table_name}/\")\n    wr.s3.to_parquet(\n        df=df,\n        path=table_path,\n        index=False,\n        dataset=True,  # True allows the other params below i.e. overwriting to db.table\n        database=db_name,\n        table=table_name,\n        mode=\"overwrite\",\n    )\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  db_name = f\"aws_example_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     print(f\"{db_name} found deleting\")     wr.catalog.delete_database(name=db_name)  # Setup source database # Create the database wr.catalog.create_database(db_name)  # Iterate through the tables in data/ and write them to our db using awswrangler for table_name in [\"department\", \"employees\", \"sales\"]:      df = pd.read_csv(f\"data/{table_name}.csv\")     table_path = os.path.join(source_db_base_path, f\"{table_name}/\")     wr.s3.to_parquet(         df=df,         path=table_path,         index=False,         dataset=True,  # True allows the other params below i.e. overwriting to db.table         database=db_name,         table=table_name,         mode=\"overwrite\",     ) In\u00a0[4]: Copied! <pre>sql_template = \"\"\"\nSELECT *\nFROM {{ db_name }}.{{ table }}\n\"\"\"\nprint(\n    pydb.render_sql_template(\n        sql_template, {\"db_name\": db_name, \"table\": \"department\"}\n    )\n)\n</pre> sql_template = \"\"\" SELECT * FROM {{ db_name }}.{{ table }} \"\"\" print(     pydb.render_sql_template(         sql_template, {\"db_name\": db_name, \"table\": \"department\"}     ) ) <pre>\nSELECT *\nFROM aws_example_mratford.department\n</pre> In\u00a0[5]: Copied! <pre># We can now use this rendered SQL to actually return the query.\nsql = pydb.render_sql_template(\n    sql_template, {\"db_name\": db_name, \"table\": \"department\"}\n)\npydb.read_sql_query(sql)\n</pre> # We can now use this rendered SQL to actually return the query. sql = pydb.render_sql_template(     sql_template, {\"db_name\": db_name, \"table\": \"department\"} ) pydb.read_sql_query(sql) Out[5]: department_id department_name 0 1 Sales 1 2 Admin 2 3 Management 3 4 Technical 4 5 Maintenance 5 6 HR In\u00a0[6]: Copied! <pre># We can also use the same template to read a different table\nsql = pydb.render_sql_template(\n    sql_template, {\"db_name\": db_name, \"table\": \"sales\"}\n)\npydb.read_sql_query(sql)\n</pre> # We can also use the same template to read a different table sql = pydb.render_sql_template(     sql_template, {\"db_name\": db_name, \"table\": \"sales\"} ) pydb.read_sql_query(sql) Out[6]: employee_id qtr sales 0 1 1 768.17 1 2 1 391.98 2 3 1 406.36 3 4 1 816.25 4 5 1 437.05 ... ... ... ... 174 43 4 442.61 175 44 4 857.64 176 45 4 644.43 177 46 4 988.18 178 59 4 927.30 <p>179 rows \u00d7 3 columns</p> In\u00a0[7]: Copied! <pre># Let's first create a an sql file\n# as your db_name is dependant on who is running this tutorial we'll use an fstring to create a basic SQL file\nwith open(\"tempfile.sql\", \"w\") as f:\n    f.write(f\"SELECT * FROM {db_name}.employees\")\n</pre> # Let's first create a an sql file # as your db_name is dependant on who is running this tutorial we'll use an fstring to create a basic SQL file with open(\"tempfile.sql\", \"w\") as f:     f.write(f\"SELECT * FROM {db_name}.employees\") <p>Now if you open up the file <code>tempfile.sql</code> you'll see a simple SQL file and jupyterlab should also give you some syntax colouring for SQL (because it recognises the <code>.sql</code> extension).</p> <p>With pydbtools you can read an SQL file and then use that query.</p> In\u00a0[8]: Copied! <pre>sql = pydb.get_sql_from_file(\"tempfile.sql\")\npydb.read_sql_query(sql)\n</pre> sql = pydb.get_sql_from_file(\"tempfile.sql\") pydb.read_sql_query(sql) Out[8]: employee_id sex forename surname department_id manager_id 0 1 M Dexter Mitchell 1.0 17.0 1 2 F Summer Bennett 1.0 17.0 2 3 M Pip Carter 1.0 17.0 3 4 F Bella Long 1.0 17.0 4 5 F Lexie Perry NaN 17.0 ... ... ... ... ... ... ... 219 223 M Austin Turner 5.0 226.0 220 224 M Theo Lewis 5.0 226.0 221 225 M William Miller NaN 226.0 222 226 F Amelie Watson 5.0 22.0 223 227 M Andy Poulton 5.0 NaN <p>224 rows \u00d7 6 columns</p> <p>We can use read in and render SQL templates. This means you can store your SQL template as a file and then just parameterise it when reading it in. Then run it.</p> <p>First lets overwrite our new file with the SQL template we originally created.</p> In\u00a0[9]: Copied! <pre># Note no f-strings this time. We are using jinja templating\nwith open(\"tempfile.sql\", \"w\") as f:\n    f.write(\"SELECT * FROM {{ db_name }}.{{ table_name }}\")\n</pre> # Note no f-strings this time. We are using jinja templating with open(\"tempfile.sql\", \"w\") as f:     f.write(\"SELECT * FROM {{ db_name }}.{{ table_name }}\") <p>Again it is worth looking at the file again (note if it looks the same, close the file in jupyter and reopen it). You should see the same SQL but with the Jinja templating.</p> <p>Now lets read in and rendor our template.</p> In\u00a0[10]: Copied! <pre>sql = pydb.get_sql_from_file(\n    \"tempfile.sql\", jinja_args={\"db_name\": db_name, \"table_name\": \"department\"}\n)\npydb.read_sql_query(sql)\n</pre> sql = pydb.get_sql_from_file(     \"tempfile.sql\", jinja_args={\"db_name\": db_name, \"table_name\": \"department\"} ) pydb.read_sql_query(sql) Out[10]: department_id department_name 0 1 Sales 1 2 Admin 2 3 Management 3 4 Technical 4 5 Maintenance 5 6 HR In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/sql_templating/#using-sql-templating-with-pydbtools","title":"Using SQL templating with pydbtools\u00b6","text":"<p>pydbtools can read SQL files and also rendor SQL with jinja templating this notebook demos both.</p> <p>Note this functionality is new to pydbtools v4.0.0.</p>"},{"location":"examples/sql_templating/#setup","title":"Setup\u00b6","text":"<p>Just run this script to create the source database so we can use it for our example.</p>"},{"location":"examples/sql_templating/#task","title":"Task\u00b6","text":"<p>We now have a database with 3 tables. We are joing to write an SQL with Jinja templating which we can \"render\" with parameters and then run that query using pydbtools.</p>"},{"location":"examples/sql_templating/#render-a-template","title":"Render a template\u00b6","text":"<p>Jinja uses the syntax of two curly brackets for it's parameters.</p>"},{"location":"examples/sql_templating/#read-and-render-a-file","title":"Read and render a file\u00b6","text":"<p>So you might be thinking this seems pointless as we can use f-strings. Which is true. But what does come into use is the ability to read in an SQL file and render it.</p>"},{"location":"examples/sql_templating/#conclusion","title":"Conclusion\u00b6","text":"<p>Writing your SQL as a file can really helpful to utilise syntax highlighting that you don't get when just writing a string. With jinja templating you can create SQL templates and then parameterise them as you please.</p> <p>Finally, when you start working with a heavy codebase of SQL across multiple users you can use tools like sqlfluff to lint your SQL files to ensure your team are working to the same standard of SQL. (SQL fluff also supports Jinja templated SQL files)</p>"},{"location":"examples/testing_new_awswrangler/","title":"Testing pydbtools with awswrangler 2.12+","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport pydbtools as pydb\n</pre> import os import pandas as pd import awswrangler as wr import pydbtools as pydb In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>bucketname = \"alpha-everyone\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\ndb_name = f\"aws_example_{foldername}\"\nsource_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(None)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"{db_name} found deleting\")\n    wr.catalog.delete_database(name=db_name)\n\n# Setup source database\n# Create the database\nwr.catalog.create_database(db_name)\n</pre> bucketname = \"alpha-everyone\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  db_name = f\"aws_example_{foldername}\" source_db_base_path = f\"s3://{bucketname}/{foldername}/source_db/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(None) if db_name in df_dbs[\"Database\"].to_list():     print(f\"{db_name} found deleting\")     wr.catalog.delete_database(name=db_name)  # Setup source database # Create the database wr.catalog.create_database(db_name) <pre>deleting objs\naws_example_mratford found deleting\n</pre> In\u00a0[4]: Copied! <pre>df = pd.DataFrame(\n    {\n        \"event\": [\n            \"Foundation of Mega-City One\",\n            \"Newspeak eclipses oldspeak\",\n            \"The year 3000\",\n        ],\n        \"future_date\": [\"2031-03-17\", \"2050-10-05\", \"3000-01-01\"],\n    }\n)\n</pre> df = pd.DataFrame(     {         \"event\": [             \"Foundation of Mega-City One\",             \"Newspeak eclipses oldspeak\",             \"The year 3000\",         ],         \"future_date\": [\"2031-03-17\", \"2050-10-05\", \"3000-01-01\"],     } ) <p>Write to a table. We need to use strings as types to get the data into the table.</p> In\u00a0[5]: Copied! <pre>table_name = \"future\"\nwr.s3.to_parquet(\n    df=df,\n    path=os.path.join(s3_base_path, table_name),\n    index=False,\n    dataset=True,\n    database=db_name,\n    table=table_name,\n    mode=\"overwrite\",\n)\n</pre> table_name = \"future\" wr.s3.to_parquet(     df=df,     path=os.path.join(s3_base_path, table_name),     index=False,     dataset=True,     database=db_name,     table=table_name,     mode=\"overwrite\", ) Out[5]: <pre>{'paths': ['s3://alpha-everyone/mratford/future/28e04de56d6240c0b03185d11dce343f.snappy.parquet'],\n 'partitions_values': {}}</pre> <p>Now use the existing table to create a new table with the date strings converted to dates.</p> In\u00a0[6]: Copied! <pre>sql = f\"\"\"\nCREATE TABLE {db_name}.new_future\nWITH (\n      external_location = '{source_db_base_path}new_future'\n)\nAS SELECT event, date(future_date) as futuredate\nFROM {db_name}.future;\n\"\"\"\nprint(sql)\n_ = pydb.start_query_execution_and_wait(sql)\n</pre> sql = f\"\"\" CREATE TABLE {db_name}.new_future WITH (       external_location = '{source_db_base_path}new_future' ) AS SELECT event, date(future_date) as futuredate FROM {db_name}.future; \"\"\" print(sql) _ = pydb.start_query_execution_and_wait(sql) <pre>\nCREATE TABLE aws_example_mratford.new_future\nWITH (\n      external_location = 's3://alpha-everyone/mratford/source_db/new_future'\n)\nAS SELECT event, date(future_date) as futuredate\nFROM aws_example_mratford.future;\n\n</pre> <p>If we query the table using a standard awswrangler query it will fail.</p> In\u00a0[7]: Copied! <pre>sql = f\"\"\"\nselect event, futuredate\nfrom {db_name}.new_future\n\"\"\"\n\ntry:\n    wr.athena.read_sql_query(sql, database=db_name, ctas_approach=False)\nexcept AttributeError:\n    print(\"Failed as expected.\")\n</pre> sql = f\"\"\" select event, futuredate from {db_name}.new_future \"\"\"  try:     wr.athena.read_sql_query(sql, database=db_name, ctas_approach=False) except AttributeError:     print(\"Failed as expected.\") <pre>Failed as expected.\n</pre> <p>A pydbtools query passes the additional parameters to awswrangler to handle the dates.</p> In\u00a0[8]: Copied! <pre>pydb.read_sql_query(sql)\n</pre> pydb.read_sql_query(sql) Out[8]: event futuredate 0 Foundation of Mega-City One 2031-03-17 1 Newspeak eclipses oldspeak 2050-10-05 2 The year 3000 3000-01-01 <p>Clean up the database and table.</p> In\u00a0[9]: Copied! <pre>pydb.delete_database_and_data(db_name)\n</pre> pydb.delete_database_and_data(db_name) Out[9]: <pre>True</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/testing_new_awswrangler/#testing-pydbtools-with-awswrangler-212","title":"Testing pydbtools with awswrangler 2.12+\u00b6","text":"<p>Later versions of awswrangler have a fix that allows large dates, e.g. the year 2500, to be processed correctly. This needs the application of additional parameters which <code>pydbtools</code> should apply automatically.</p>"},{"location":"examples/testing_new_awswrangler/#setup","title":"Setup\u00b6","text":""},{"location":"examples/testing_new_awswrangler/#create-a-dataframe-with-difficult-dates","title":"Create a dataframe with difficult dates\u00b6","text":""},{"location":"examples/updating_db_with_deltas/","title":"Updating DB with Deltas","text":"In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport awswrangler as wr\nimport datetime\nimport pydbtools as pydb\nfrom scripts.create_dummy_deltas import get_dummy_deltas\n</pre> import os import pandas as pd import awswrangler as wr import datetime import pydbtools as pydb from scripts.create_dummy_deltas import get_dummy_deltas In\u00a0[2]: Copied! <pre># setup your own testing area (set foldername = GH username)\nfoldername = \"mratford\"  # GH username\nfoldername = foldername.lower().replace(\"-\", \"_\")\n</pre> # setup your own testing area (set foldername = GH username) foldername = \"mratford\"  # GH username foldername = foldername.lower().replace(\"-\", \"_\") In\u00a0[3]: Copied! <pre>region = \"eu-west-1\"\nbucketname = \"alpha-everyone\"\ndb_name = f\"aws_example_{foldername}\"\ndb_base_path = f\"s3://{bucketname}/{foldername}/database\"\ns3_base_path = f\"s3://{bucketname}/{foldername}/\"\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(limit=1000)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(f\"deleting database {db_name}\")\n    wr.catalog.delete_database(name=db_name)\n</pre> region = \"eu-west-1\" bucketname = \"alpha-everyone\" db_name = f\"aws_example_{foldername}\" db_base_path = f\"s3://{bucketname}/{foldername}/database\" s3_base_path = f\"s3://{bucketname}/{foldername}/\"  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(limit=1000) if db_name in df_dbs[\"Database\"].to_list():     print(f\"deleting database {db_name}\")     wr.catalog.delete_database(name=db_name) In\u00a0[4]: Copied! <pre>deltas = get_dummy_deltas(\"data/employees.csv\")\n</pre> deltas = get_dummy_deltas(\"data/employees.csv\") <p>Day 1: the first extract of deltas from our databases</p> In\u00a0[5]: Copied! <pre>deltas[\"day1\"]\n</pre> deltas[\"day1\"] Out[5]: employee_id sex forename surname department_id manager_id record_deleted 0 1 M Dexter Mitchell 1 17 False 1 2 F Summer Bennett 1 17 False 2 3 M Pip Carter 1 17 False 3 4 F Bella Long 1 17 False 4 5 F Lexie Perry &lt;NA&gt; 17 False <p>Day 2: The next days deltas show that Lexie has their <code>department_id</code> and <code>manager_id</code> corrected. As well 2 new employees.</p> In\u00a0[6]: Copied! <pre>deltas[\"day2\"]\n</pre> deltas[\"day2\"] Out[6]: employee_id sex forename surname department_id manager_id record_deleted 0 5 F Lexie Perry 2 18 False 1 6 M Robert Roberts 1 17 False 2 7 F Iris Alexander 1 17 False <p>Day 3: The next days deltas show that:</p> <ul> <li>Dexter has left the department</li> <li>Robert and Iris have moved departments and are working for Lexie</li> <li>3 New employees are also now working for Lexie</li> </ul> In\u00a0[7]: Copied! <pre>deltas[\"day3\"]\n</pre> deltas[\"day3\"] Out[7]: employee_id sex forename surname department_id manager_id record_deleted 0 1 M Dexter Mitchell 1 17 True 1 7 F Iris Alexander 2 5 False 2 9 M Evan Carter 2 5 False 3 10 F Lauren Powell 2 5 False 4 11 F Alice James 2 5 False In\u00a0[8]: Copied! <pre># Init database and delta table\nwr.catalog.create_database(name=db_name)\n\n# Add some parameters that will be useful to manage our deltas\ndf = deltas[\"day1\"]\ndf[\"date_received\"] = datetime.date(2021, 1, 1)\n\n# We are going to name the folder the same as our table\n# this makes things less complex and is adviced\ntable_name = \"raw_deltas\"\nraw_delta_path = os.path.join(db_base_path, table_name)\n_ = wr.s3.to_parquet(\n    df,\n    path=raw_delta_path,\n    dataset=True,\n    database=db_name,\n    table=table_name,\n    mode=\"append\",\n)\n</pre> # Init database and delta table wr.catalog.create_database(name=db_name)  # Add some parameters that will be useful to manage our deltas df = deltas[\"day1\"] df[\"date_received\"] = datetime.date(2021, 1, 1)  # We are going to name the folder the same as our table # this makes things less complex and is adviced table_name = \"raw_deltas\" raw_delta_path = os.path.join(db_base_path, table_name) _ = wr.s3.to_parquet(     df,     path=raw_delta_path,     dataset=True,     database=db_name,     table=table_name,     mode=\"append\", ) In\u00a0[9]: Copied! <pre>sql = f\"SELECT * FROM {db_name}.{table_name}\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False)\n</pre> sql = f\"SELECT * FROM {db_name}.{table_name}\" print(sql) pydb.read_sql_query(sql, ctas_approach=False) <pre>SELECT * FROM aws_example_mratford.raw_deltas\n</pre> Out[9]: employee_id sex forename surname department_id manager_id record_deleted date_received 0 1 M Dexter Mitchell 1 17 False 2021-01-01 1 2 F Summer Bennett 1 17 False 2021-01-01 2 3 M Pip Carter 1 17 False 2021-01-01 3 4 F Bella Long 1 17 False 2021-01-01 4 5 F Lexie Perry &lt;NA&gt; 17 False 2021-01-01 In\u00a0[10]: Copied! <pre>def create_report_pandas(report_date):\n    # Read in data with pandas\n    df = wr.s3.read_parquet(raw_delta_path)\n\n    # Turn input into a date\n    d = datetime.datetime.strptime(report_date, \"%Y-%m-%d\").date()\n\n    # filter records after specified date\n    df = df[df.date_received &lt;= d].reset_index(drop=True)\n\n    # Get the latest records per employee_id\n    latest_df = (\n        df.sort_values(\n            [\"employee_id\", \"date_received\"], ascending=[True, False]\n        )\n        .groupby(\"employee_id\")\n        .head(1)\n    )\n\n    # Remove deleted records\n    latest_df = latest_df[~latest_df[\"record_deleted\"]].reset_index(drop=True)\n    latest_df[\"report_date\"] = d\n\n    # Remove date_received col\n    latest_df = latest_df.drop(columns=[\"date_received\", \"record_deleted\"])\n\n    # Write dataframe to new table partitioned by report_date\n    table_name = \"employee_pandas\"\n    table_path = os.path.join(db_base_path, table_name)\n\n    # Write the data to S3 but only overwrite partitions\n    _ = wr.s3.to_parquet(\n        latest_df,\n        path=table_path,\n        dataset=True,\n        database=db_name,\n        table=table_name,\n        partition_cols=[\"report_date\"],\n        mode=\"overwrite_partitions\",\n    )\n</pre> def create_report_pandas(report_date):     # Read in data with pandas     df = wr.s3.read_parquet(raw_delta_path)      # Turn input into a date     d = datetime.datetime.strptime(report_date, \"%Y-%m-%d\").date()      # filter records after specified date     df = df[df.date_received &lt;= d].reset_index(drop=True)      # Get the latest records per employee_id     latest_df = (         df.sort_values(             [\"employee_id\", \"date_received\"], ascending=[True, False]         )         .groupby(\"employee_id\")         .head(1)     )      # Remove deleted records     latest_df = latest_df[~latest_df[\"record_deleted\"]].reset_index(drop=True)     latest_df[\"report_date\"] = d      # Remove date_received col     latest_df = latest_df.drop(columns=[\"date_received\", \"record_deleted\"])      # Write dataframe to new table partitioned by report_date     table_name = \"employee_pandas\"     table_path = os.path.join(db_base_path, table_name)      # Write the data to S3 but only overwrite partitions     _ = wr.s3.to_parquet(         latest_df,         path=table_path,         dataset=True,         database=db_name,         table=table_name,         partition_cols=[\"report_date\"],         mode=\"overwrite_partitions\",     ) In\u00a0[11]: Copied! <pre># Run code to create report for 2021-01-01 data\ncreate_report_pandas(\"2021-01-01\")\n</pre> # Run code to create report for 2021-01-01 data create_report_pandas(\"2021-01-01\") <pre>/home/jovyan/.cache/pypoetry/virtualenvs/pydbtools-VGEYFVdX-py3.9/lib/python3.9/site-packages/awswrangler/s3/_write_dataset.py:92: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for keys, subgroup in df.groupby(by=partition_cols, observed=True):\n</pre> <p>We have created a report based on all the deltas up to and including <code>2021-01-01</code> (which at the moment is only one delta)</p> In\u00a0[12]: Copied! <pre>sql = f\"SELECT * FROM {db_name}.employee_pandas\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False)\n</pre> sql = f\"SELECT * FROM {db_name}.employee_pandas\" print(sql) pydb.read_sql_query(sql, ctas_approach=False) <pre>SELECT * FROM aws_example_mratford.employee_pandas\n</pre> Out[12]: employee_id sex forename surname department_id manager_id report_date 0 1 M Dexter Mitchell 1 17 2021-01-01 1 2 F Summer Bennett 1 17 2021-01-01 2 3 M Pip Carter 1 17 2021-01-01 3 4 F Bella Long 1 17 2021-01-01 4 5 F Lexie Perry &lt;NA&gt; 17 2021-01-01 In\u00a0[13]: Copied! <pre>def create_report_athena(report_date, ctas):\n\n    table_path = os.path.join(db_base_path, \"employee_athena\")\n\n    # Clear out the partition we are going to write to\n    s3_partition_path = os.path.join(table_path, f\"report_date={report_date}\")\n    wr.s3.delete_objects(s3_partition_path)\n\n    # Actual logic in SQL to create the report\n    sql = f\"\"\"\n    SELECT employee_id,\n        sex,\n        forename,\n        surname,\n        department_id,\n        manager_id,\n        date '{report_date}' AS report_date    \n    FROM\n    (\n        SELECT *,\n        row_number() OVER (PARTITION BY employee_id ORDER BY date_received DESC) as rn\n        FROM {db_name}.raw_deltas\n        WHERE date_received &lt;= date '{report_date}'\n    )\n    WHERE NOT record_deleted AND rn = 1\n    \"\"\"\n\n    # If ctas is true then create table for the\n    # first time otherwise use an insert into query\n    # put the original SQL one of the below\n    if ctas:\n        # Creating table for the first time\n        full_sql = f\"\"\"\n        CREATE TABLE {db_name}.employee_athena \n        WITH (\n          external_location = '{table_path}',\n          partitioned_by = ARRAY['report_date']\n        ) AS\n        {sql}\n        \"\"\"\n    else:\n        full_sql = f\"\"\"\n        INSERT INTO {db_name}.employee_athena\n        {sql}\n        \"\"\"\n\n    # run the query\n    pydb.start_query_execution_and_wait(full_sql)\n</pre> def create_report_athena(report_date, ctas):      table_path = os.path.join(db_base_path, \"employee_athena\")      # Clear out the partition we are going to write to     s3_partition_path = os.path.join(table_path, f\"report_date={report_date}\")     wr.s3.delete_objects(s3_partition_path)      # Actual logic in SQL to create the report     sql = f\"\"\"     SELECT employee_id,         sex,         forename,         surname,         department_id,         manager_id,         date '{report_date}' AS report_date         FROM     (         SELECT *,         row_number() OVER (PARTITION BY employee_id ORDER BY date_received DESC) as rn         FROM {db_name}.raw_deltas         WHERE date_received &lt;= date '{report_date}'     )     WHERE NOT record_deleted AND rn = 1     \"\"\"      # If ctas is true then create table for the     # first time otherwise use an insert into query     # put the original SQL one of the below     if ctas:         # Creating table for the first time         full_sql = f\"\"\"         CREATE TABLE {db_name}.employee_athena          WITH (           external_location = '{table_path}',           partitioned_by = ARRAY['report_date']         ) AS         {sql}         \"\"\"     else:         full_sql = f\"\"\"         INSERT INTO {db_name}.employee_athena         {sql}         \"\"\"      # run the query     pydb.start_query_execution_and_wait(full_sql) In\u00a0[14]: Copied! <pre># Run code to create report for 2021-01-01 data\ncreate_report_athena(\"2021-01-01\", ctas=True)\n</pre> # Run code to create report for 2021-01-01 data create_report_athena(\"2021-01-01\", ctas=True) In\u00a0[15]: Copied! <pre>sql = f\"SELECT * FROM {db_name}.employee_athena\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False)\n</pre> sql = f\"SELECT * FROM {db_name}.employee_athena\" print(sql) pydb.read_sql_query(sql, ctas_approach=False) <pre>SELECT * FROM aws_example_mratford.employee_athena\n</pre> Out[15]: employee_id sex forename surname department_id manager_id report_date 0 1 M Dexter Mitchell 1 17 2021-01-01 1 4 F Bella Long 1 17 2021-01-01 2 3 M Pip Carter 1 17 2021-01-01 3 2 F Summer Bennett 1 17 2021-01-01 4 5 F Lexie Perry &lt;NA&gt; 17 2021-01-01 In\u00a0[16]: Copied! <pre>df = deltas[\"day2\"]\ndf[\"date_received\"] = datetime.date(2021, 1, 2)\n\n_ = wr.s3.to_parquet(\n    df,\n    path=raw_delta_path,\n    dataset=True,\n    database=db_name,\n    table=table_name,\n    mode=\"append\",\n)\n</pre> df = deltas[\"day2\"] df[\"date_received\"] = datetime.date(2021, 1, 2)  _ = wr.s3.to_parquet(     df,     path=raw_delta_path,     dataset=True,     database=db_name,     table=table_name,     mode=\"append\", ) <p>The run the reports for the same date (now the deltas table has been updated)</p> In\u00a0[17]: Copied! <pre>create_report_pandas(\"2021-01-02\")\ncreate_report_athena(\"2021-01-02\", ctas=False)  # note we use insert to now\n</pre> create_report_pandas(\"2021-01-02\") create_report_athena(\"2021-01-02\", ctas=False)  # note we use insert to now <pre>/home/jovyan/.cache/pypoetry/virtualenvs/pydbtools-VGEYFVdX-py3.9/lib/python3.9/site-packages/awswrangler/s3/_write_dataset.py:92: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for keys, subgroup in df.groupby(by=partition_cols, observed=True):\n</pre> In\u00a0[18]: Copied! <pre>sql = f\"\"\"\nSELECT *\nFROM {db_name}.employee_athena\nWHERE report_date = date '2021-01-02'\n\"\"\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False)\n</pre> sql = f\"\"\" SELECT * FROM {db_name}.employee_athena WHERE report_date = date '2021-01-02' \"\"\" print(sql) pydb.read_sql_query(sql, ctas_approach=False) <pre>\nSELECT *\nFROM aws_example_mratford.employee_athena\nWHERE report_date = date '2021-01-02'\n\n</pre> Out[18]: employee_id sex forename surname department_id manager_id report_date 0 5 F Lexie Perry 2 18 2021-01-02 1 7 F Iris Alexander 1 17 2021-01-02 2 2 F Summer Bennett 1 17 2021-01-02 3 4 F Bella Long 1 17 2021-01-02 4 6 M Robert Roberts 1 17 2021-01-02 5 1 M Dexter Mitchell 1 17 2021-01-02 6 3 M Pip Carter 1 17 2021-01-02 <p>As we can see new employyes have been added and Lexie's department and manager records have been updated as expected.</p> <p>It is also worth noting that previous reports have been untouched (using the pandas table as an example)</p> In\u00a0[19]: Copied! <pre>sql = f\"\"\"\nSELECT *\nFROM {db_name}.employee_pandas\n\"\"\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False)\n</pre> sql = f\"\"\" SELECT * FROM {db_name}.employee_pandas \"\"\" print(sql) pydb.read_sql_query(sql, ctas_approach=False) <pre>\nSELECT *\nFROM aws_example_mratford.employee_pandas\n\n</pre> Out[19]: employee_id sex forename surname department_id manager_id report_date 0 1 M Dexter Mitchell 1 17 2021-01-02 1 2 F Summer Bennett 1 17 2021-01-02 2 3 M Pip Carter 1 17 2021-01-02 3 4 F Bella Long 1 17 2021-01-02 4 5 F Lexie Perry 2 18 2021-01-02 5 6 M Robert Roberts 1 17 2021-01-02 6 7 F Iris Alexander 1 17 2021-01-02 7 1 M Dexter Mitchell 1 17 2021-01-01 8 2 F Summer Bennett 1 17 2021-01-01 9 3 M Pip Carter 1 17 2021-01-01 10 4 F Bella Long 1 17 2021-01-01 11 5 F Lexie Perry &lt;NA&gt; 17 2021-01-01 In\u00a0[20]: Copied! <pre># update raw deltas first\ndf = deltas[\"day3\"]\ndf[\"date_received\"] = datetime.date(2021, 1, 3)\n\n_ = wr.s3.to_parquet(\n    df,\n    path=raw_delta_path,\n    dataset=True,\n    database=db_name,\n    table=table_name,\n    mode=\"append\",\n)\n\n# Then run reports\ncreate_report_pandas(\"2021-01-03\")\ncreate_report_athena(\"2021-01-03\", ctas=False)\n</pre> # update raw deltas first df = deltas[\"day3\"] df[\"date_received\"] = datetime.date(2021, 1, 3)  _ = wr.s3.to_parquet(     df,     path=raw_delta_path,     dataset=True,     database=db_name,     table=table_name,     mode=\"append\", )  # Then run reports create_report_pandas(\"2021-01-03\") create_report_athena(\"2021-01-03\", ctas=False) <pre>/home/jovyan/.cache/pypoetry/virtualenvs/pydbtools-VGEYFVdX-py3.9/lib/python3.9/site-packages/awswrangler/s3/_write_dataset.py:92: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n  for keys, subgroup in df.groupby(by=partition_cols, observed=True):\n</pre> In\u00a0[21]: Copied! <pre>sql = f\"\"\"\nSELECT *\nFROM {db_name}.employee_pandas\nWHERE report_date = date '2021-01-03'\n\"\"\"\nprint(sql)\npydb.read_sql_query(sql, ctas_approach=False)\n</pre> sql = f\"\"\" SELECT * FROM {db_name}.employee_pandas WHERE report_date = date '2021-01-03' \"\"\" print(sql) pydb.read_sql_query(sql, ctas_approach=False) <pre>\nSELECT *\nFROM aws_example_mratford.employee_pandas\nWHERE report_date = date '2021-01-03'\n\n</pre> Out[21]: employee_id sex forename surname department_id manager_id report_date 0 2 F Summer Bennett 1 17 2021-01-03 1 3 M Pip Carter 1 17 2021-01-03 2 4 F Bella Long 1 17 2021-01-03 3 5 F Lexie Perry 2 18 2021-01-03 4 6 M Robert Roberts 1 17 2021-01-03 5 7 F Iris Alexander 2 5 2021-01-03 6 9 M Evan Carter 2 5 2021-01-03 7 10 F Lauren Powell 2 5 2021-01-03 8 11 F Alice James 2 5 2021-01-03 <p>From the above we can see that Dexter has been removed from the report (as he left) and new staff have been added. Again as expected when looking at our original deltas.</p> In\u00a0[22]: Copied! <pre>### Clean up\n\n# Delete all the s3 files in a given path\nif wr.s3.list_objects(s3_base_path):\n    print(\"deleting objs\")\n    wr.s3.delete_objects(s3_base_path)\n\n# Delete the database if it exists\ndf_dbs = wr.catalog.databases(limit=1000)\nif db_name in df_dbs[\"Database\"].to_list():\n    print(\"Deleting database\")\n    wr.catalog.delete_database(name=db_name)\n</pre> ### Clean up  # Delete all the s3 files in a given path if wr.s3.list_objects(s3_base_path):     print(\"deleting objs\")     wr.s3.delete_objects(s3_base_path)  # Delete the database if it exists df_dbs = wr.catalog.databases(limit=1000) if db_name in df_dbs[\"Database\"].to_list():     print(\"Deleting database\")     wr.catalog.delete_database(name=db_name) <pre>deleting objs\nDeleting database\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"examples/updating_db_with_deltas/#updating-db-with-deltas","title":"Updating DB with Deltas\u00b6","text":"<p>In this tutorial we are going to demonstrate how to make a database based on deltas recieved from an external source. We will build a database containing a table of all the raw deltas and then create a second database that shows us the state of the raw table delta at a particular date.</p> <p>We are going to pretend that we recieve a <code>csv</code> file that contains changes of a table. We are going to concatenate those deltas into a single table. Then generate a subsequent table based on the \"raw\" deltas. This latter part we will do twice once using Pandas and once using Athena.</p>"},{"location":"examples/updating_db_with_deltas/#setup-first","title":"Setup first\u00b6","text":""},{"location":"examples/updating_db_with_deltas/#get-the-deltas","title":"Get the deltas\u00b6","text":"<p>We are going to create deltas from the <code>\"data/employees.csv</code> table. I am using code in a script in this repo <code>scripts/create_dummy_deltas.py</code>. It isn't important what it is doing for this tutorial but if you wanna see what it does you can.</p>"},{"location":"examples/updating_db_with_deltas/#create-a-database-and-tables","title":"Create a database and tables\u00b6","text":"<p>There are many ways you can create a database and tables (see other tutorials). For this example I am going to do this in the simplest way - using awswrangler (which infers the table schema from the data).</p> <p>If you want to explicitly specify the schema look at the <code>data_conformance_and_dbs</code> tutorial.</p>"},{"location":"examples/updating_db_with_deltas/#take-stock","title":"Take stock\u00b6","text":"<p>We now have a database that we created once and we initialised our <code>raw_deltas</code> table in our database.</p> <p>Now we are going to create two tables (that will have the same table). Both these tables will show what our raw_deltas will look like at each day we do an update (the first table will be created using pandas the other using Athena).</p> <p>We are also going to wrap these code chunks into functions. This will help us utilise these functions later to show how you can run a delta update and then the downstream tables</p>"},{"location":"examples/updating_db_with_deltas/#pandas-derived-table","title":"Pandas derived table\u00b6","text":"<p>We are going to read in all the data in our s3 path and create a new df from that then write it to a new table.</p>"},{"location":"examples/updating_db_with_deltas/#athena-derived-table","title":"Athena derived table\u00b6","text":"<p>We are going to do the same thing we did with pandas, but this time using Athena. You cannot delete partitions in Athena (because athena just queries your data in S3 using SQL) it doesn't actually alter anything that already exists in S3. But we can predetermine where our tables will sit so we can just delete the S3 path first before writing a partition in there each time.</p>"},{"location":"examples/updating_db_with_deltas/#final-bit","title":"Final bit\u00b6","text":"<p>Now we have 3 tables.</p> <ul> <li><code>raw_deltas</code> a table of all the raw data concatenated</li> <li><code>employee_athena</code> a report based on what employees table looked like at a given <code>report_date</code>. (Remember in this example the raw_deltas are from an external table employees where we get given daily deltas of changes).</li> <li><code>employee_pandas</code> The same report as employee_athena but using pandas instead of athena to create it.</li> </ul> <p>Now we want to update each of these tables based on the data from day2 then do it again for day3s data. Lets do that now (starting with day 2)</p>"},{"location":"examples/updating_db_with_deltas/#day2","title":"Day2\u00b6","text":"<p>Add day2 data to the deltas table</p>"},{"location":"examples/updating_db_with_deltas/#day-3","title":"Day 3\u00b6","text":"<p>Lets run the same again for day 3. The code is exactly the same as it was for day2 but now with a new date</p>"},{"location":"examples/updating_db_with_deltas/#wrapping-up","title":"Wrapping Up\u00b6","text":"<p>So hopefully that is useful. Some further notes and thoughts</p>"},{"location":"examples/updating_db_with_deltas/#using-pandas-vs-athena-for-reports","title":"Using pandas vs Athena for reports\u00b6","text":"<p>Just use what you feel is most confortable for your team they each have pros and cons</p>"},{"location":"examples/updating_db_with_deltas/#partitioning","title":"Partitioning\u00b6","text":"<p>Normally you want to partition your data to reduce query size with the report above the SQL query will only look in a specific S3 path as you filtered the table on a partition column. However, partitioning on small chunks of data (like we have above) can actually reduce performance of your athena query. The upside is that it makes it easier to know where your data sits in S3 (i.e. look how we predetermine the s3 path for a partition in a table in the <code>create_report_athena</code> function).</p> <p>The above example didn't actually need partitioning, you could just append the data each time (like what is done for the <code>raw_deltas</code> table). The question to ask is how much you think it will effect performance / how difficult will it be to return the data to a previous state if you made a mistake. I.e. I can delete a partition very easily and know I am not going to delete any other data but that is less clear when appending data to the same folder. On the flip side it is trivial to delete everything in AWS for this tutorial and run it from scratch as the data and transforms are minimal so rolling back changes to a previous state wouldn't be that hard to do. Ultimately the decision is up to you.</p>"},{"location":"examples/scripts/__init__/","title":"init","text":""},{"location":"examples/scripts/create_dummy_deltas/","title":"Create dummy deltas","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>def get_dummy_deltas(employees_path):\n    employees = pd.read_csv(employees_path, nrows=10)\n\n    # Create deleted flag\n    employees[\"record_deleted\"] = False\n    employees[\"record_deleted\"] = employees[\"record_deleted\"].astype(\n        pd.BooleanDtype()\n    )\n\n    # Cast to new int cols\n    for col in [\"employee_id\", \"department_id\", \"manager_id\"]:\n        employees[col] = employees[col].astype(pd.Int64Dtype())\n\n    # Cast to new str cols\n    for col in [\"sex\", \"forename\", \"surname\"]:\n        employees[col] = employees[col].astype(pd.StringDtype())\n\n    # Let's split up the data and make some changes\n    day1 = employees[employees.employee_id.isin([1, 2, 3, 4, 5])].reset_index(\n        drop=True\n    )\n\n    day2 = employees[employees.employee_id.isin([5, 6, 7])].reset_index(\n        drop=True\n    )\n    day2.loc[0, \"department_id\"] = 2\n    day2.loc[0, \"manager_id\"] = 18\n\n    day3 = employees[\n        employees.employee_id.isin([1, 7, 9, 10, 11])\n    ].reset_index(drop=True)\n    day3.department_id = 2\n    day3.manager_id = 5\n\n    # Reset this persons values for clarity\n    day3.loc[0, \"record_deleted\"] = True\n    day3.loc[0, \"department_id\"] = 1\n    day3.loc[0, \"manager_id\"] = 17\n\n    deltas = {\"day1\": day1, \"day2\": day2, \"day3\": day3}\n\n    return deltas\n</pre> def get_dummy_deltas(employees_path):     employees = pd.read_csv(employees_path, nrows=10)      # Create deleted flag     employees[\"record_deleted\"] = False     employees[\"record_deleted\"] = employees[\"record_deleted\"].astype(         pd.BooleanDtype()     )      # Cast to new int cols     for col in [\"employee_id\", \"department_id\", \"manager_id\"]:         employees[col] = employees[col].astype(pd.Int64Dtype())      # Cast to new str cols     for col in [\"sex\", \"forename\", \"surname\"]:         employees[col] = employees[col].astype(pd.StringDtype())      # Let's split up the data and make some changes     day1 = employees[employees.employee_id.isin([1, 2, 3, 4, 5])].reset_index(         drop=True     )      day2 = employees[employees.employee_id.isin([5, 6, 7])].reset_index(         drop=True     )     day2.loc[0, \"department_id\"] = 2     day2.loc[0, \"manager_id\"] = 18      day3 = employees[         employees.employee_id.isin([1, 7, 9, 10, 11])     ].reset_index(drop=True)     day3.department_id = 2     day3.manager_id = 5      # Reset this persons values for clarity     day3.loc[0, \"record_deleted\"] = True     day3.loc[0, \"department_id\"] = 1     day3.loc[0, \"manager_id\"] = 17      deltas = {\"day1\": day1, \"day2\": day2, \"day3\": day3}      return deltas"},{"location":"guide/overview/","title":"Overview","text":""},{"location":"guide/overview/#introduction","title":"Introduction","text":"<p>This package is a wrapper for awswrangler that which presets/defines some of the input parameters to the athena module functions to align with our platform setup. See the awswrangler API reference documentation for Athena to see what functions you can call from pydbtools.</p> <p>The function parameters that are locked down / altered by <code>pydbtools</code> are:</p> <ul> <li> <p>boto3_session: This is auto generated by <code>pydbtools</code> (in order to grab the user credentials from the sts client - this is needed for the R version of this package which calls this package under the hood. In short forcing refreshed credentials are needed in R as boto3 credentials timeout and do not refresh when using reticulate, though this does not apply to the latest version of the platform currently being rolled out.)</p> </li> <li> <p>s3_output: The S3 path where database queries are written to. This is defined by <code>pydbtools</code> based on the IAM user/role calling the query (ensures that each role can only read/write to a S3 path only they can access).</p> </li> <li> <p>database: Will either be set to <code>None</code> or <code>__temp__</code> depending on other user parameters (if <code>ctas_approach=True</code>). <code>__temp__</code> is an alias to an autogenerated temp database name which is generated from <code>pydbtools</code> again based on the IAM user/role. References to this temporary database can be referenced by the keyword <code>__temp__</code> in SQL queries see additional functionality to awswrangler section.</p> </li> <li> <p>sql: We allows reference to the database name <code>__temp__</code> which is an alias to a user specific temporary database. When a function call has an SQL parameter the SQL is checked with an SQL parser and then any reference to <code>__temp__</code> as a database is replaced with the actual database name which is autogenerated. This replacement only occurs for <code>SELECT</code> queries.</p> </li> <li> <p>pyarrow_additional_kwargs: This is set to <code>{\"coerce_int96_timestamp_unit\": \"ms\", \"timestamp_as_object\": True}</code> by default. Doing this solves this awswrangler issue)</p> </li> </ul>"},{"location":"guide/overview/#functionality","title":"Functionality","text":"<p>As well as acting as a wrapper function for awswrangler this package also allows you to do the following:</p>"},{"location":"guide/overview/#run-query-and-wait-for-a-response","title":"Run query and wait for a response","text":"<p>This function essentially calls two functions from <code>awswrangler.athena</code>. First <code>start_query_execution</code> followed by <code>wait_query</code>.</p> <pre><code>import pydbtools as pydb\n\nresponse = pydb.start_query_execution_and_wait(\"SELECT * from a_database.table LIMIT 10\")\n</code></pre>"},{"location":"guide/overview/#create-temporary-tables","title":"Create Temporary Tables","text":"<p>You can use the <code>create_temp_table</code> function to write SQL to create a store a temporary table that sits in your <code>__temp__</code> database.</p> <pre><code>import pydbtools as pydb\n\npydb.create_temp_table(\"SELECT * from a_database.table LIMIT 10\", table_name=\"temp_table_1\")\ndf = pydb.read_sql_query(\"SELECT * from __temp__.temp_table_1\")\ndf.head()\n</code></pre> <p>See the example notebook for a more detailed example.</p>"},{"location":"guide/overview/#create-databases-and-tables","title":"Create databases and tables","text":"<pre><code>import pydbtools as pydb\nimport pandas as pd\n\npydb.create_database(\"my_db\")\npydb.file_to_table(\n    \"local_file_path/data.csv\",\n    database=\"my_db\",\n    table=\"my_table\",\n    location=\"s3://my_s3_location/my_table\"\n)\npydb.dataframe_to_table(\n    my_dataframe,\n    database=\"my_db\",\n    table=\"my_other_table\",\n    location=\"s3://my_s3_location/my_other_table\"\n)\npydb.create_table(\n    \"select * from my_db.my_other_table where month = 'March'\",\n    database=\"my_db\",\n    table=\"my_march_table\",\n    location=\"s3://my_s3_location/my_other_table\"\n)\n</code></pre> <p>See the notebook on MoJAP tools for more details.</p>"},{"location":"guide/overview/#run-sql-from-a-string-of-statements-or-a-file","title":"Run SQL from a string of statements or a file","text":"<p>It wil often be more convenient to write your SQL in an editor with language support rather than as a Python string. You can create temporary tables within SQL using the syntax below.</p> <pre><code>import pydbtools as pydb\n\nsql = \"\"\"\ncreate temp table A as (\n    select * from database.table1\n    where year = 2021\n);\n\ncreate temp table B as (\n    select * from database.table2\n    where amount &gt; 10\n);\n\nselect * from __temp__.A\nleft join __temp__.B\non A.id = B.id;\n\"\"\"\n\nwith open(\"queries.sql\", \"w\") as f:\n    f.write(sql)\n\nwith open(\"queries.sql\", \"r\") as f:\n    df = pydb.read_sql_queries(f.read())\n</code></pre> <p>Multiple <code>SELECT</code> queries can be returned as a generator of dataframes using <code>read_sql_queries_gen</code>.</p> <p>See the notebook on creating temporary tables with SQL and the notebook on database administration with SQL for more detailed examples.</p> <p>Additionally you can use Jinja templating to inject arguments into your SQL.</p> <pre><code>sql_template = \"\"\"\nSELECT *\nFROM {{ db_name }}.{{ table }}\n\"\"\"\nsql = pydb.render_sql_template(sql_template, {\"db_name\": db_name, \"table\": \"department\"})\npydb.read_sql_query(sql)\n\nwith open(\"tempfile.sql\", \"w\") as f:\n    f.write(\"SELECT * FROM {{ db_name }}.{{ table_name }}\")\nsql = pydb.get_sql_from_file(\"tempfile.sql\", jinja_args={\"db_name\": db_name, \"table_name\": \"department\"})\npydb.read_sql_query(sql)\n\"\"\"\n</code></pre> <p>See the notebook on SQL templating for more details.</p>"},{"location":"guide/overview/#delete-databases-tables-and-partitions-together-with-the-data-on-s3","title":"Delete databases, tables and partitions together with the data on S3","text":"<pre><code>import pydbtools as pydb\n\npydb.delete_partitions_and_data(database='my_database', table='my_table', expression='year = 2020 or year = 2021')\npydb.delete_table_and_data(database='my_database', table='my_table')\npydb.delete_database('my_database')\n\n# These can be used for temporary databases and tables.\npydb.delete_table_and_data(database='__temp__', table='my_temp_table')\n</code></pre> <p>For more details see the notebook on deletions.</p>"},{"location":"guide/overview/#examples","title":"Examples","text":""},{"location":"guide/overview/#simple","title":"Simple","text":"<pre><code>import pydbtools as pydb\n\n# Run a query using pydbtools\nresponse = pydb.start_query_execution_and_wait(\"CREATE DATABASE IF NOT EXISTS my_test_database\")\n\n# Read data from an athena query directly into pandas\npydb.read_sql(\"SELECT * from a_database.table LIMIT 10\")\n\n# Create a temp table to do further seperate SQL queries later on\npydb.create_temp_table(\"SELECT a_col, count(*) as n FROM a_database.table GROUP BY a_col\", table_name=\"temp_table_1\")\ndf = pydb.read_sql_query(\"SELECT * FROM __temp__.temp_table_1 WHERE n &lt; 10\")\n</code></pre>"},{"location":"guide/overview/#more-advanced-usage","title":"More advanced usage","text":"<p>Get the actual name for your temp database, create your temp db then delete it using awswrangler (note: <code>awswrangler</code> will raise an error if the database does not exist)</p> <pre><code>import awswrangler as wr\nimport pydbtools as pydb\n\nuser_id, out_path = pydb.get_user_id_and_table_dir()\ntemp_db_name = pydb.get_database_name_from_userid(user_id)\nprint(temp_db_name)\npydb.create_temp_table()\nprint(wr.catalog.delete_database(name=temp_db_name))\n</code></pre>"}]}