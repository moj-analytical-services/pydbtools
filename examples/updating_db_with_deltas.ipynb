{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "first-aberdeen",
   "metadata": {},
   "source": [
    "# Updating DB with Deltas\n",
    "\n",
    "In this tutorial we are going to demonstrate how to make a database based on deltas recieved from an external source. We will build a database containing a table of all the raw deltas and then create a second database that shows us the state of the raw table delta at a particular date.\n",
    "\n",
    "We are going to pretend that we recieve a `csv` file that contains changes of a table. We are going to concatenate those deltas into a single table. Then generate a subsequent table based on the \"raw\" deltas. This latter part we will do twice once using Pandas and once using Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-wiring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import datetime\n",
    "import pydbtools as pydb\n",
    "from scripts.create_dummy_deltas import get_dummy_deltas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driven-speaker",
   "metadata": {},
   "source": [
    "## Setup first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your own testing area (set foldername = GH username)\n",
    "foldername = \"mratford\"  # GH username\n",
    "foldername = foldername.lower().replace(\"-\", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "region = \"eu-west-1\"\n",
    "bucketname = \"alpha-everyone\"\n",
    "db_name = f\"aws_example_{foldername}\"\n",
    "db_base_path = f\"s3://{bucketname}/{foldername}/database\"\n",
    "s3_base_path = f\"s3://{bucketname}/{foldername}/\"\n",
    "\n",
    "# Delete all the s3 files in a given path\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)\n",
    "\n",
    "# Delete the database if it exists\n",
    "df_dbs = wr.catalog.databases(limit=1000)\n",
    "if db_name in df_dbs[\"Database\"].to_list():\n",
    "    print(f\"deleting database {db_name}\")\n",
    "    wr.catalog.delete_database(name=db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-ideal",
   "metadata": {},
   "source": [
    "### Get the deltas\n",
    "\n",
    "We are going to create deltas from the `\"data/employees.csv` table. I am using code in a script in this repo `scripts/create_dummy_deltas.py`. It isn't important what it is doing for this tutorial but if you wanna see what it does you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas = get_dummy_deltas(\"data/employees.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-acquisition",
   "metadata": {},
   "source": [
    "**Day 1:** the first extract of deltas from our databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-toolbox",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas[\"day1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-raleigh",
   "metadata": {},
   "source": [
    "**Day 2:** The next days deltas show that Lexie has their `department_id` and `manager_id` corrected. As well 2 new employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-ground",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas[\"day2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-verse",
   "metadata": {},
   "source": [
    "**Day 3:** The next days deltas show that:\n",
    "- Dexter has left the department\n",
    "- Robert and Iris have moved departments and are working for Lexie\n",
    "- 3 New employees are also now working for Lexie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-passenger",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltas[\"day3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-mediterranean",
   "metadata": {},
   "source": [
    "###Â Create a database and tables\n",
    "\n",
    "There are many ways you can create a database and tables (see other tutorials). For this example I am going to do this in the simplest way - using awswrangler (which infers the table schema from the data).\n",
    "\n",
    "> If you want to explicitly specify the schema look at the `data_conformance_and_dbs` tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bright-convenience",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init database and delta table\n",
    "wr.catalog.create_database(name=db_name)\n",
    "\n",
    "# Add some parameters that will be useful to manage our deltas\n",
    "df = deltas[\"day1\"]\n",
    "df[\"date_received\"] = datetime.date(2021, 1, 1)\n",
    "\n",
    "# We are going to name the folder the same as our table\n",
    "# this makes things less complex and is adviced\n",
    "table_name = \"raw_deltas\"\n",
    "raw_delta_path = os.path.join(db_base_path, table_name)\n",
    "_ = wr.s3.to_parquet(\n",
    "    df,\n",
    "    path=raw_delta_path,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table=table_name,\n",
    "    mode=\"append\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-success",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"SELECT * FROM {db_name}.{table_name}\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-trader",
   "metadata": {},
   "source": [
    "### Take stock\n",
    "\n",
    "We now have a database that we created once and we initialised our `raw_deltas` table in our database.\n",
    "\n",
    "Now we are going to create two tables (that will have the same table). Both these tables will show what our raw_deltas will look like at each day we do an update (the first table will be created using pandas the other using Athena).\n",
    "\n",
    "> We are also going to wrap these code chunks into functions. This will help us utilise these functions later to show how you can run a delta update and then the downstream tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-ethernet",
   "metadata": {},
   "source": [
    "### Pandas derived table\n",
    "\n",
    "We are going to read in all the data in our s3 path and create a new df from that then write it to a new table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-directory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_pandas(report_date):\n",
    "    # Read in data with pandas\n",
    "    df = wr.s3.read_parquet(raw_delta_path)\n",
    "\n",
    "    # Turn input into a date\n",
    "    d = datetime.datetime.strptime(report_date, \"%Y-%m-%d\").date()\n",
    "\n",
    "    # filter records after specified date\n",
    "    df = df[df.date_received <= d].reset_index(drop=True)\n",
    "\n",
    "    # Get the latest records per employee_id\n",
    "    latest_df = (\n",
    "        df.sort_values(\n",
    "            [\"employee_id\", \"date_received\"], ascending=[True, False]\n",
    "        )\n",
    "        .groupby(\"employee_id\")\n",
    "        .head(1)\n",
    "    )\n",
    "\n",
    "    # Remove deleted records\n",
    "    latest_df = latest_df[~latest_df[\"record_deleted\"]].reset_index(drop=True)\n",
    "    latest_df[\"report_date\"] = d\n",
    "\n",
    "    # Remove date_received col\n",
    "    latest_df = latest_df.drop(columns=[\"date_received\", \"record_deleted\"])\n",
    "\n",
    "    # Write dataframe to new table partitioned by report_date\n",
    "    table_name = \"employee_pandas\"\n",
    "    table_path = os.path.join(db_base_path, table_name)\n",
    "\n",
    "    # Write the data to S3 but only overwrite partitions\n",
    "    _ = wr.s3.to_parquet(\n",
    "        latest_df,\n",
    "        path=table_path,\n",
    "        dataset=True,\n",
    "        database=db_name,\n",
    "        table=table_name,\n",
    "        partition_cols=[\"report_date\"],\n",
    "        mode=\"overwrite_partitions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code to create report for 2021-01-01 data\n",
    "create_report_pandas(\"2021-01-01\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-hobby",
   "metadata": {},
   "source": [
    "We have created a report based on all the deltas up to and including `2021-01-01` (which at the moment is only one delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-quantity",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"SELECT * FROM {db_name}.employee_pandas\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-lighter",
   "metadata": {},
   "source": [
    "### Athena derived table\n",
    "\n",
    "We are going to do the same thing we did with pandas, but this time using Athena. You cannot delete partitions in Athena (because athena just queries your data in S3 using SQL) it doesn't actually alter anything that already exists in S3. But we can predetermine where our tables will sit so we can just delete the S3 path first before writing a partition in there each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_report_athena(report_date, ctas):\n",
    "\n",
    "    table_path = os.path.join(db_base_path, \"employee_athena\")\n",
    "\n",
    "    # Clear out the partition we are going to write to\n",
    "    s3_partition_path = os.path.join(table_path, f\"report_date={report_date}\")\n",
    "    wr.s3.delete_objects(s3_partition_path)\n",
    "\n",
    "    # Actual logic in SQL to create the report\n",
    "    sql = f\"\"\"\n",
    "    SELECT employee_id,\n",
    "        sex,\n",
    "        forename,\n",
    "        surname,\n",
    "        department_id,\n",
    "        manager_id,\n",
    "        date '{report_date}' AS report_date    \n",
    "    FROM\n",
    "    (\n",
    "        SELECT *,\n",
    "        row_number() OVER (PARTITION BY employee_id ORDER BY date_received DESC) as rn\n",
    "        FROM {db_name}.raw_deltas\n",
    "        WHERE date_received <= date '{report_date}'\n",
    "    )\n",
    "    WHERE NOT record_deleted AND rn = 1\n",
    "    \"\"\"\n",
    "\n",
    "    # If ctas is true then create table for the\n",
    "    # first time otherwise use an insert into query\n",
    "    # put the original SQL one of the below\n",
    "    if ctas:\n",
    "        # Creating table for the first time\n",
    "        full_sql = f\"\"\"\n",
    "        CREATE TABLE {db_name}.employee_athena \n",
    "        WITH (\n",
    "          external_location = '{table_path}',\n",
    "          partitioned_by = ARRAY['report_date']\n",
    "        ) AS\n",
    "        {sql}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        full_sql = f\"\"\"\n",
    "        INSERT INTO {db_name}.employee_athena\n",
    "        {sql}\n",
    "        \"\"\"\n",
    "\n",
    "    # run the query\n",
    "    pydb.start_query_execution_and_wait(full_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-electronics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run code to create report for 2021-01-01 data\n",
    "create_report_athena(\"2021-01-01\", ctas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"SELECT * FROM {db_name}.employee_athena\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electric-satellite",
   "metadata": {},
   "source": [
    "### Final bit\n",
    "\n",
    "Now we have 3 tables.\n",
    "\n",
    "- `raw_deltas` a table of all the raw data concatenated\n",
    "- `employee_athena` a report based on what employees table looked like at a given `report_date`. (Remember in this example the raw_deltas are from an external table employees where we get given daily deltas of changes).\n",
    "- `employee_pandas` The same report as employee_athena but using pandas instead of athena to create it.\n",
    "\n",
    "Now we want to update each of these tables based on the data from day2 then do it again for day3s data. Lets do that now (starting with day 2)\n",
    "\n",
    "### Day2\n",
    "\n",
    "Add day2 data to the deltas table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deltas[\"day2\"]\n",
    "df[\"date_received\"] = datetime.date(2021, 1, 2)\n",
    "\n",
    "_ = wr.s3.to_parquet(\n",
    "    df,\n",
    "    path=raw_delta_path,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table=table_name,\n",
    "    mode=\"append\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-transaction",
   "metadata": {},
   "source": [
    "The run the reports for the same date (now the deltas table has been updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_report_pandas(\"2021-01-02\")\n",
    "create_report_athena(\"2021-01-02\", ctas=False)  # note we use insert to now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gross-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM {db_name}.employee_athena\n",
    "WHERE report_date = date '2021-01-02'\n",
    "\"\"\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-immune",
   "metadata": {},
   "source": [
    "As we can see new employyes have been added and Lexie's department and manager records have been updated as expected.\n",
    "\n",
    "It is also worth noting that previous reports have been untouched (using the pandas table as an example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM {db_name}.employee_pandas\n",
    "\"\"\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-cable",
   "metadata": {},
   "source": [
    "### Day 3\n",
    "\n",
    "Lets run the same again for day 3. The code is exactly the same as it was for day2 but now with a new date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update raw deltas first\n",
    "df = deltas[\"day3\"]\n",
    "df[\"date_received\"] = datetime.date(2021, 1, 3)\n",
    "\n",
    "_ = wr.s3.to_parquet(\n",
    "    df,\n",
    "    path=raw_delta_path,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table=table_name,\n",
    "    mode=\"append\",\n",
    ")\n",
    "\n",
    "# Then run reports\n",
    "create_report_pandas(\"2021-01-03\")\n",
    "create_report_athena(\"2021-01-03\", ctas=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hourly-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT *\n",
    "FROM {db_name}.employee_pandas\n",
    "WHERE report_date = date '2021-01-03'\n",
    "\"\"\"\n",
    "print(sql)\n",
    "pydb.read_sql_query(sql, ctas_approach=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-northeast",
   "metadata": {},
   "source": [
    "From the above we can see that Dexter has been removed from the report (as he left) and new staff have been added. Again as expected when looking at our original deltas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-relations",
   "metadata": {},
   "source": [
    "### Wrapping Up\n",
    "\n",
    "So hopefully that is useful. Some further notes and thoughts\n",
    "\n",
    "#### Using pandas vs Athena for reports\n",
    "\n",
    "Just use what you feel is most confortable for your team they each have pros and cons\n",
    "\n",
    "#### Partitioning\n",
    "\n",
    "Normally you want to partition your data to reduce query size with the report above the SQL query will only look in a specific S3 path as you filtered the table on a partition column. However, partitioning on small chunks of data (like we have above) can actually reduce performance of your athena query. The upside is that it makes it easier to know where your data sits in S3 (i.e. look how we predetermine the s3 path for a partition in a table in the `create_report_athena` function).\n",
    "\n",
    "The above example didn't actually need partitioning, you could just append the data each time (like what is done for the `raw_deltas` table). The question to ask is how much you think it will effect performance / how difficult will it be to return the data to a previous state if you made a mistake. I.e. I can delete a partition very easily and know I am not going to delete any other data but that is less clear when appending data to the same folder. On the flip side it is trivial to delete everything in AWS for this tutorial and run it from scratch as the data and transforms are minimal so rolling back changes to a previous state wouldn't be that hard to do. Ultimately the decision is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-truck",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up\n",
    "\n",
    "# Delete all the s3 files in a given path\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)\n",
    "\n",
    "# Delete the database if it exists\n",
    "df_dbs = wr.catalog.databases(limit=1000)\n",
    "if db_name in df_dbs[\"Database\"].to_list():\n",
    "    print(\"Deleting database\")\n",
    "    wr.catalog.delete_database(name=db_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pediatric-making",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydbtools",
   "language": "python",
   "name": "pydbtools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
