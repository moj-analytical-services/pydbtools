{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Database Creation and Manipulation\n",
    "\n",
    "In this tutorial we are going to use aws-wrangler to create a database of different tables.\n",
    "\n",
    "Let's create a database out of the test data `employees.csv`, `sales.csv` and `department.csv` (all in the `data/` folder)\n",
    "\n",
    "Note this is basically taken from: https://github.com/awslabs/aws-data-wrangler/blob/master/tutorials/014%20-%20Schema%20Evolution.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import datetime\n",
    "import pydbtools as pydb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your own testing area (set foldername = GH username)\n",
    "foldername = \"mratford\" # GH username\n",
    "foldername = foldername.lower().replace(\"-\",\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketname = \"alpha-everyone\"\n",
    "db_name = f\"aws_example_{foldername}\"\n",
    "db_base_path = f\"s3://{bucketname}/{foldername}/database\"\n",
    "s3_base_path = f\"s3://{bucketname}/{foldername}/\"\n",
    "\n",
    "# Delete all the s3 files in a given path\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)\n",
    "\n",
    "# Delete the database if it exists\n",
    "df_dbs = wr.catalog.databases(None)\n",
    "if db_name in df_dbs[\"Database\"].to_list():\n",
    "    wr.catalog.delete_database(\n",
    "        name=db_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get the data in pandas first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/employees.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets do some transforms on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"creation_date\"] = datetime.date(2021, 1, 1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write the table to a database\n",
    "\n",
    "parquet is always your best bet for writing data to a Glue Database especially if you only want to retrieve that data via Athena SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the database\n",
    "wr.catalog.create_database(db_name)\n",
    "\n",
    "# note table_path is a folder as glue treats all the\n",
    "# data in a folder as contents of a single table\n",
    "table_path = f\"{db_base_path}/employees/\"\n",
    "\n",
    "# Write your pandas dataframe to S3 and add it as a table in your database\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=table_path,\n",
    "    index=False,\n",
    "    dataset=True, # True allows the other params below i.e. overwriting to db.table\n",
    "    database=db_name,\n",
    "    table='employees',\n",
    "    mode=\"overwrite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append new data to the table\n",
    "\n",
    "Let's for fun also add new cols as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"creation_date\"] = datetime.date(2021, 1, 1)\n",
    "\n",
    "df[\"new_col1\"] = df[\"employee_id\"] + 100\n",
    "df[\"new_col2\"] = \"some text\"\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the new data to S3.\n",
    "# Note the only thing has changed is mode=\"append\" whereas previously it was mode=\"overwrite\"\n",
    "wr.s3.to_parquet(\n",
    "    df=df,\n",
    "    path=table_path,\n",
    "    index=False,\n",
    "    dataset=True,\n",
    "    database=db_name,\n",
    "    table='employees',\n",
    "    mode=\"append\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now query the data with Athena to look at it\n",
    "\n",
    "This should use pydbtools rather than aws_wrangler (if you are a AP user)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each uploaded dataset had one employee with an employee_id == 1\n",
    "# So lets pull that down to demonstrate both tables were added to the data\n",
    "sql = f\"SELECT * from {db_name}.employees where employee_id = 1\"\n",
    "db_table = pydb.read_sql_query(\n",
    "    sql,\n",
    "    ctas_approach=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clean up\n",
    "\n",
    "# Delete all the s3 files in a given path\n",
    "if wr.s3.list_objects(s3_base_path):\n",
    "    print(\"deleting objs\")\n",
    "    wr.s3.delete_objects(s3_base_path)\n",
    "\n",
    "# Delete the database if it exists\n",
    "df_dbs = wr.catalog.databases(None)\n",
    "if db_name in df_dbs[\"Database\"].to_list():\n",
    "    print(f\"deleting {db_name}\")\n",
    "    wr.catalog.delete_database(\n",
    "        name=db_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate db no longer exists\n",
    "db_name in wr.catalog.databases()[\"Database\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python, pydbtools",
   "language": "python",
   "name": "pydbtools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
