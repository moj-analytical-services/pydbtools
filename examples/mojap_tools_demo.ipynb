{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c2127f2-09d1-4b59-ac92-2b2daa4a54c2",
   "metadata": {},
   "source": [
    "# MoJ AP tools demo\n",
    "\n",
    "This notebook demonstrates the use of some of the Python tools developed by data engineers to make creating analytical pipelines simpler for data scientists and analysts.\n",
    "\n",
    "It focuses on taking a large dataset which is too big for memory, converting it to another format while applying metadata to ensure consistent data types, and creating a database with tables from files or dataframes.\n",
    "\n",
    "First import the necessary libraries. [pydbtools](https://github.com/moj-analytical-services/pydbtools), [arrow_pd_parser](https://github.com/moj-analytical-services/mojap-arrow-pd-parser) and [mojap_metadata](https://github.com/moj-analytical-services/mojap-metadata) are libraries created and maintained by the Data Modelling and Engineering Team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05413085-1381-4988-b217-df3281d264b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydbtools as pydb\n",
    "from arrow_pd_parser import reader, writer\n",
    "from mojap_metadata import Metadata\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "import itertools\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6201de-5eb5-4160-9643-b3052adba4c5",
   "metadata": {},
   "source": [
    "Create a new database, cleaning up any tables and data beforehand in case it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f93875-6f79-43f3-bf25-9c586e9b8338",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = \"dmet_example\"\n",
    "\n",
    "pydb.delete_database_and_data(db)\n",
    "pydb.create_database(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc32a85-4d71-4a49-bd3e-ed3a981acde8",
   "metadata": {},
   "source": [
    "We have a dataset that consists of a number of very large csv files. How can we load this without running out of memory and crashing our session?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687b8d18-5f09-49da-b65f-bc9728528441",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_path = \"s3://alpha-everyone/s3_data_packer_test/land/big/\"\n",
    "\n",
    "# Don't run this!\n",
    "# df = wr.s3.read_csv(big_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb568ec-94c3-43ce-bb8c-82c3cb325dd1",
   "metadata": {},
   "source": [
    "`arrow_pd_parser` has the ability to read files in chunks, returning an iterator of dataframes. Specify a number of lines to load with chunksize to preview the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55ddb5c-06c1-4545-9bc0-2b50f6bee875",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(reader.read(big_path, file_format=\"csv\", chunksize=10, index_col=0))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8c7d11-667a-4f26-a977-33a98726b610",
   "metadata": {},
   "source": [
    "Checking the data types we can see that `date_time` is a string but we would like it to be a timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6921e14-a82d-4301-a778-7d5bdc3090fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4e2ecc-f1e8-4015-b994-82bbb4931ef4",
   "metadata": {},
   "source": [
    "Create metadata to fix this using `mojap_metadata.Metadata`. Note that `arrow` rather than `pandas` types are used, and these will be enforced across formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d06a602-09a1-444c-a614-4991c17ea893",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = Metadata.from_dict(\n",
    "    {\n",
    "        \"name\": \"big_table\",\n",
    "        \"columns\": [\n",
    "            { \"name\": n, \"type\": t }\n",
    "            for n, t in [\n",
    "                (\"name\", \"string\"),\n",
    "                (\"email\", \"string\"),\n",
    "                (\"address\", \"string\"),\n",
    "                (\"city\", \"string\"),\n",
    "                (\"state\", \"string\"),\n",
    "                (\"date_time\", \"timestamp(ms)\"),\n",
    "                (\"price\", \"int64\")\n",
    "            ]\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "metadata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f940c5-419c-412b-a53e-b131f16a242a",
   "metadata": {},
   "source": [
    "Now try previewing the data again with metadata enforced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f95dd-bf72-4588-9c50-a5be2765426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = next(reader.read(\n",
    "    big_path, \n",
    "    file_format=\"csv\", \n",
    "    chunksize=10,\n",
    "    index_col=0, \n",
    "    metadata=metadata\n",
    "))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fc7251-d46b-4f9b-bd3a-bf4d79dbd1c6",
   "metadata": {},
   "source": [
    "Note that `date_time` is now an object of `datetime.datetime` type as the `pandas` date/time types have too narrow a range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cf707-4062-4a14-a7cf-407639090544",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da63e3-7e92-4487-be0a-c17340548f0d",
   "metadata": {},
   "source": [
    "For the sake of this demo take a small slice, the first 5 chunks, of an iterator reading the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67a255f-dd32-4365-a660-f2d99796567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = itertools.islice(\n",
    "    reader.read(\n",
    "        big_path, \n",
    "        file_format=\"csv\", \n",
    "        chunksize=\"100MB\", \n",
    "        index_col=0, \n",
    "        metadata=metadata\n",
    "    ),\n",
    "    5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3651bb08-4840-4a7b-9246-d21d0f3ef977",
   "metadata": {},
   "source": [
    "We can then convert between formats, in this case to parquet, while preserving the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ca7d5-787b-4c94-94c2-58ed88c408bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path = \"s3://alpha-everyone/dmet_st/big_table.parquet\"\n",
    "wr.s3.delete_objects(new_path)\n",
    "writer.write(r, new_path, metadata=metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d56d0c-55a3-4a23-9aa1-0948ef922c5c",
   "metadata": {},
   "source": [
    "Big datasets in S3 can be used to create a queryable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d869e-ffe9-4da2-841e-6fedbdb86dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydb.file_to_table(\n",
    "    new_path,\n",
    "    database=db,\n",
    "    table=\"big_table\",\n",
    "    location=\"s3://alpha-everyone/dmet_st/dmet_example/big_table\",\n",
    "    chunksize=\"100MB\",\n",
    "    metadata=metadata\n",
    ")\n",
    "pydb.read_sql_query(f\"select * from {db}.big_table limit 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce68eb6-aa13-437d-91ec-21cd38d3eec5",
   "metadata": {},
   "source": [
    "Create a new table in the database from an SQL statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20fbac2-298c-47c3-a309-03a2fa2aebdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydb.delete_table_and_data(database=db, table=\"state_revenues\")\n",
    "\n",
    "pydb.create_table(\n",
    "    f\"\"\"\n",
    "    select state, sum(price) as revenue\n",
    "    from {db}.big_table\n",
    "    group by state\n",
    "    \"\"\",\n",
    "    database=db,\n",
    "    table=\"state_revenues\",\n",
    "    location=\"s3://alpha-everyone/dmet_st/dmet_example/state_revenues\"\n",
    ")\n",
    "\n",
    "sr = pydb.read_sql_query(f\"select * from {db}.state_revenues\")\n",
    "sr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9414d7a-0450-44b7-ab51-a344a35ee151",
   "metadata": {},
   "source": [
    "What if we want to do some manipulation with pandas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06efef3d-74ab-4f0a-8fed-674761c6750c",
   "metadata": {},
   "outputs": [],
   "source": [
    "starts_with_n = sr[sr[\"state\"].str.startswith(\"N\")]\n",
    "starts_with_n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a565cf1-370f-4bb5-9ac0-7a5f23ee44da",
   "metadata": {},
   "source": [
    "We can then create another table in the database from the manipulated dataframe. This allows us to create a hybrid pipeline of SQL and pandas operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659dd652-7efa-407f-a640-93c63d7d5c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "pydb.dataframe_to_table(\n",
    "    starts_with_n,\n",
    "    database=db,\n",
    "    table=\"starts_with_n\",\n",
    "    location=\"s3://alpha-everyone/dmet_st/dmet_example/starts_with_n\"\n",
    ")\n",
    "\n",
    "pydb.read_sql_query(f\"select * from {db}.starts_with_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e144c8-bb4b-4e34-82a8-6d83e1f3497d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydbtools",
   "language": "python",
   "name": "pydbtools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
